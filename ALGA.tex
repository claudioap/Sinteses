\documentclass[]{report}
\usepackage[a4paper, total={7in, 8.5in}]{geometry}
\usepackage[portuguese]{babel}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[dvipsnames]{xcolor}
\usepackage{forest}
\usepackage{tikz}
\usetikzlibrary{calc,matrix}
\tikzset{
  treenode/.style = {align=center},
  root/.style     = {treenode, font=\Large},
  env/.style      = {treenode, font=\normalsize},
  dummy/.style    = {circle,draw}
}

\begin{document}
\begin{titlepage}
	\centering
	\vspace{5cm}
	{\huge\bfseries Álgebra Linear e Geometria Analítica\par}
	\vspace{1cm}
	{\scshape\Large Síntese baseada no conteúdo lecionado na\\
	 FCT/Universidade Nova de Lisboa\par}
	\vspace{2cm}
	Adaptado por:\\
	{\Large \textit{Cláudio Afonso de Sousa Pereira}\\
	(sinteses$\text{@}$claudiop$.$com)\par}
	\vspace{1cm}
	{\large \today\par}
	\vfill
	Adaptação licenciada:\\
	\href{http://creativecommons.org/licenses/by-sa/4.0/}{\includegraphics[scale=0.8]{ccbysa.png}}
\end{titlepage}
\chapter{Calculo Matricial}
\section{Matrizes}
Matrizes são estruturas compostas por diversos números em grelha, tendo propriedades e operações próprias. São úteis a certas áreas de estudo podendo ajudar a abstrair problemas.\\
Uma matriz $m$ por $n$ (interpretada "matriz com $m$ linhas e $n$ colunas") tem o formato:
$$\begin{bmatrix}
a_{11} & a_{12} & \dots & a_{1n} \\
a_{21} & a_{22} & \dots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \dots & a_{mn}
\end{bmatrix}$$
Assim sendo, $a_{ij}$ é dito elemento na linha $i$ e coluna $j$.\\
Os elementos pode pertencer aos reais ($\mathbb{R}$) ou aos complexos ($\mathbb{C}$).\\
Se $\forall i,j : a_{ij} \in \mathbb{R}$ a matriz acima é declarada $\mathcal{M}_{m \times n} \in \mathbb{R}$.\\
\subsection{Matrizes notáveis}
Alguns formatos matrizes são especialmente importantes:
\begin{itemize}
\item \textbf{Quadrada} - Toda a que tenha o mesmo numero de linhas e colunas.\\
$\mathcal{M}_{n \times n}$ - Quadrada da ordem $n$, abreviada $\mathcal{M}_{n}$.\\
A \textbf{diagonal principal} de uma matriz são os elementos entre o canto superior esquerdo e o inferior direito.
\item \textbf{Triangular} - Quadrada nulificada de um dos lados da diagonal principal.
$$\begin{bmatrix}
a_{11} & \dots & a_{1n} \\
0 & \ddots & \vdots \\
0 & 0 & a_{mn}
\end{bmatrix}$$
\item \textbf{Diagonal} - Quadrada que só tem elementos não-nulos na diagonal principal.
$$\begin{bmatrix}
a_{11} & 0 & 0 \\
0 & \ddots & 0\\
0 & 0 & a_{mn}
\end{bmatrix}$$
\item \textbf{Escalar} - Diagonal em que todos os elementos da diagonal principal tem o mesmo valor.
$$\begin{bmatrix}
k & 0 & 0 \\
0 & \ddots & 0\\
0 & 0 & k
\end{bmatrix}$$
\item \textbf{Identidade} - Quadrada com os elementos da diagonal principal $=1$.
$$\begin{bmatrix}
1 & 0 & 0 \\
0 & \ddots & 0\\
0 & 0 & 1
\end{bmatrix}$$
\item \textbf{Nula} - Constituída por elementos nulos.
$$\begin{bmatrix}
0 & \dots & 0 \\
\vdots & \ddots & \vdots\\
0 & \dots & 1
\end{bmatrix}$$
\item \textbf{Linha} - Apenas tem uma linha.
$$\begin{bmatrix}
a_{11} & \dots & a_{1n}
\end{bmatrix}$$
\item \textbf{Coluna} - Apenas tem uma coluna.
$$\begin{bmatrix}
a_{1} \\
\vdots \\
a_{m1}
\end{bmatrix}$$
\end{itemize} 
\section{Operações aritméticas}
\subsection{Adição}
Duas matrizes $A$ e $B$ podem somadas da seguinte forma:
$$A+B = 
\begin{bmatrix}
\color{red}a_{11} & \color{red}a_{12} & \dots & \color{red}a_{1n} \\
\color{red}a_{21} & \color{red}a_{22} & \dots & \color{red}a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
\color{red}a_{m1} & \color{red}a_{m2} & \dots & \color{red}a_{mn}
\end{bmatrix}
+
\begin{bmatrix}
\color{blue}b_{11} & \color{blue}b_{12} & \dots & \color{blue}b_{1n} \\
\color{blue}b_{21} & \color{blue}b_{22} & \dots & \color{blue}b_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
\color{blue}b_{m1} & \color{blue}b_{m2} & \dots & \color{blue}b_{mn}
\end{bmatrix}
=
\begin{bmatrix}
\textcolor{red}{a_{11}}+\textcolor{blue}{b_{11}} & \textcolor{red}{a_{12}}+\textcolor{blue}{b_{12}} & \dots & \textcolor{red}{a_{1n}}+\textcolor{blue}{b_{1n}} \\
\textcolor{red}{a_{21}}+\textcolor{blue}{b_{21}} & \textcolor{red}{a_{22}}+\textcolor{blue}{b_{22}} & \dots & \textcolor{red}{a_{2n}}+\textcolor{blue}{b_{2n}} \\
\vdots & \vdots & \ddots & \vdots \\
\textcolor{red}{a_{m1}}+\textcolor{blue}{b_{m1}} & \textcolor{red}{a_{m2}}+\textcolor{blue}{b_{m2}} & \dots & \textcolor{red}{a_{mn}}+\textcolor{blue}{b_{mn}}
\end{bmatrix}$$
A adição de matrizes é comutativa ($A+B = B+A$) e associativa ($A+(B+C) = (A+B)+C$).
\subsection{Produto escalar}
$$
\textcolor{red}{\alpha}
\begin{bmatrix}
\color{blue}b_{11} & \color{blue}b_{12} & \dots & \color{blue}b_{1n} \\
\color{blue}b_{21} & \color{blue}b_{22} & \dots & \color{blue}b_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
\color{blue}b_{m1} & \color{blue}b_{m2} & \dots & \color{blue}b_{mn}
\end{bmatrix}
=
\begin{bmatrix}
\textcolor{red}{\alpha}+\textcolor{blue}{b_{11}} & \textcolor{red}{\alpha}+\textcolor{blue}{b_{12}} & \dots & \textcolor{red}{\alpha}+\textcolor{blue}{b_{1n}} \\
\textcolor{red}{\alpha}+\textcolor{blue}{b_{21}} & \textcolor{red}{\alpha}+\textcolor{blue}{b_{22}} & \dots & \textcolor{red}{\alpha}+\textcolor{blue}{b_{2n}} \\
\vdots & \vdots & \ddots & \vdots \\
\textcolor{red}{\alpha}+\textcolor{blue}{b_{m1}} & \textcolor{red}{\alpha}+\textcolor{blue}{b_{m2}} & \dots & \textcolor{red}{\alpha}+\textcolor{blue}{b_{mn}}
\end{bmatrix}$$
\begin{itemize}
\item $\alpha(A+B) = \alpha A + \alpha B$
\item $(\alpha + \beta)A = \alpha A + \beta A$
\item $(\alpha\beta)A = \alpha(\beta A)$
\end{itemize}
\subsection{Produto}
O produto de matrizes requer que a esquerda seja $\mathcal{M}_{m\times k}$ e a direita $\mathcal{M}_{k\times n}$.\\
Como o numero de colunas da primeira tem que ser igual ao numero de linhas da segunda intuitivamente o produto de matrizes \underline{não é comutativo} (nem quando ambas são quadradas).
$$(AB)_{ij} = \sum^n_{k=1} a_{ik}b_{kj}$$
Isto é, o elemento $(AB)_{i,j}$ da matriz resultante é dado por:
$$AB = 
\begin{bmatrix}
\vdots & \vdots & \vdots & \vdots \\
\color{red}a_{i1} & \color{red}a_{i2} & \dots & \color{red}a_{in} \\
\vdots & \vdots & \vdots & \vdots
\end{bmatrix}
\times
\begin{bmatrix}
\dots & \color{blue}b_{12} & \dots \\
\dots & \color{blue}b_{22} & \dots\\
\vdots & \vdots & \vdots \\
\dots & \color{blue}b_{mj} & \dots
\end{bmatrix}
=
\begin{bmatrix}
\dots & \dots & \dots\\
\dots & \textcolor{red}{a_{i1}}\textcolor{blue}{b_{1j}} + \textcolor{red}{a_{i2}}\textcolor{blue}{b_{2j}} + \dots + \textcolor{red}{a_{in}}\textcolor{blue}{b_{nj}} & \dots\\
\dots & \dots & \dots
\end{bmatrix}$$
\begin{itemize}
\item $(AB)C = A(BC)$ (Associativa)
\item $A(B+C) = AB + AC, \quad (B+C)A = BA + CA$ (Distributiva, á esquerda e direita)
\item $\alpha(AB) = (\alpha A)B = A(\alpha B)$ (Multiplicação por escalar)
\item $AI_n = I_m A = A$ (Multiplicação por identidade)
\item Se $AB=AC$ ou $BA = CA$, sabendo que $A \neq 0$, sabe-se que $B=C$.
\end{itemize}
\subsection{Potência}
$A^k
\begin{cases}
I_n&,\text{se } k=1\\
A^{k-1}A&,\text{se } k=\mathbb{N}
\end{cases}
$\\
$A^k A^l = A^{k+l}$\\
$A^{k^l} = A^{kl}$
\section{Transformações Elementares}
A partir das operações aritméticas em matrizes, há operações compostas que são definidas como transformações elementares por linhas (ou por colunas). São as seguintes (quando por linhas):
\begin{enumerate}
\item \textbf{Troca de linhas}
$$
\begin{bmatrix}
a_{11} & a_{12} & \dots & a_{1n}\\
\dots & \dots & \dots & \dots\\
\color{red}a_{i1} & \color{red}a_{i2} & \dots & \color{red}a_{in}\\
\dots & \dots & \dots & \dots\\
\color{blue}a_{j1} & \color{blue}a_{j2} & \dots & \color{blue}a_{jn}\\
\dots & \dots & \dots & \dots\\
a_{11} & a_{12} & \dots & a_{1n}\\
\end{bmatrix}
\overrightarrow{l_i \leftrightarrow l_j}
\begin{bmatrix}
a_{11} & a_{12} & \dots & a_{1n}\\
\dots & \dots & \dots & \dots\\
\color{blue}a_{j1} & \color{blue}a_{j2} & \dots & \color{blue}a_{jn}\\
\dots & \dots & \dots & \dots\\
\color{red}a_{i1} & \color{red}a_{i2} & \dots & \color{red}a_{in}\\
\dots & \dots & \dots & \dots\\
a_{11} & a_{12} & \dots & a_{1n}\\
\end{bmatrix}
$$
\item \textbf{Multiplicação a linha}
$$
\begin{bmatrix}
a_{11} & a_{12} & \dots & a_{1n}\\
\dots & \dots & \dots & \dots\\
\color{red}a_{i1} & \color{red}a_{i2} & \dots & \color{red}a_{in}\\
\dots & \dots & \dots & \dots\\
a_{11} & a_{12} & \dots & a_{1n}\\
\end{bmatrix}
\overrightarrow{\textcolor{blue}{\alpha} l_i}
\begin{bmatrix}
a_{11} & a_{12} & \dots & a_{1n}\\
\dots & \dots & \dots & \dots\\
\color{blue}\alpha\color{red}a_{i1} & \color{blue}\alpha\color{red}a_{i2} & \dots & \color{blue}\alpha\color{red}a_{in}\\
\dots & \dots & \dots & \dots\\
a_{11} & a_{12} & \dots & a_{1n}\\
\end{bmatrix}
$$
\textbf{Nota}: é necessário garantir que $\alpha \neq 0$.
\item \textbf{Soma com múltiplo de outra linha}
$$
\begin{bmatrix}
a_{11} & a_{12} & \dots & a_{1n}\\
\dots & \dots & \dots & \dots\\
\color{red}a_{i1} & \color{red}a_{i2} & \dots & \color{red}a_{in}\\
\dots & \dots & \dots & \dots\\
\color{blue}a_{j1} & \color{blue}a_{j2} & \dots & \color{blue}a_{jn}\\
\dots & \dots & \dots & \dots\\
a_{11} & a_{12} & \dots & a_{1n}\\
\end{bmatrix}
\overrightarrow{l_i + \textcolor{green}{\beta} l_j}
\begin{bmatrix}
a_{11} & a_{12} & \dots & a_{1n}\\
\dots & \dots & \dots & \dots\\
\textcolor{red}{a_{i1}}+ \color{green}\beta\color{blue}a_{j1} & \textcolor{red}{a_{i2}}+ \color{green}\beta\color{blue}a_{j2} & \dots & \textcolor{red}{a_{in}}+ \color{green}\beta\color{blue}a_{jn}\\
\dots & \dots & \dots & \dots\\
\color{blue}a_{j1} & \color{blue}a_{j2} & \dots & \color{blue}a_{jn}\\
\dots & \dots & \dots & \dots\\
a_{11} & a_{12} & \dots & a_{1n}\\
\end{bmatrix}
$$
\end{enumerate}
Como dito, as mesmas operações podem ser feitas por colunas.\\[0.5cm]
Seja $A \in \mathcal{M}_{m \times n}$ e $\alpha, \beta \in \mathbb{R}$.
\begin{itemize}
\item $A {\scriptstyle{\overrightarrow{\quad T \quad}}} B$ representa que $B$ resulta de uma transformação elementar, do tipo $T$, aplicada a $A$.
\item $A$ é \textbf{equivalente por linhas} a $B$ se um numero finito de transformações elementares por linhas em $A$ resultar em $B$.
\end{itemize}
\subsection{Matrizes elementares e formas de escada}
Á matriz que resulte da aplicação de \underline{uma única} transformação elementar a uma matriz identidade, designa-se de \textbf{matriz elementar}.
\begin{itemize}
\item Uma matriz tem uma transformação por linhas quando pré-multiplicada por uma matriz elementar.
\item Uma matriz tem uma transformação por colunas quando pós-multiplicada por uma matriz elementar.
\item A inversa de uma matriz elementar reverte a operação que a elementar aplica.
\end{itemize}
Seja o elemento \textbf{pivô} de uma linha o elemento não nulo mais à esquerda dessa linha e que não apresenta mais nenhum elemento não nulo por baixo de si (na coluna).
Quando uma matriz apresenta um pivô em todas as linhas não nulas, e quaisquer eventuais linhas nulas venham depois das não nulas, então a matriz tem \textbf{forma de escada}.\\[0.2cm]
O número de linhas não nulas de uma matriz em escada é chamado de \textbf{caraterística}.
A caraterística da matriz $A$ representa-se $r(A)$ e é sempre menor ou igual tanto ao numero de linhas como de colunas.\\[0.2cm]
Verificam-se as seguintes propriedades:
\begin{itemize}
\item Toda a matriz é equivalente por linhas a uma matriz em forma de escada.
\item Se uma matriz  escada contiver 1 como elemento pivô de cada coluna, e todos os restantes elementos dessa coluna forem nulos, a matriz é dita em forma de \textbf{escada reduzida} e é única. 
\end{itemize}
\section{Invertíbilidade}
Sejam $A, B \in \mathcal{M}_{n}$ (matrizes quadradas), $\alpha \in \mathbb{R}$ 
(ou $\mathbb{C}$) e $k \in \mathbb{N}$.\\
A matriz $A \in \mathcal{M}_{n}$ é invertível se existe uma matriz $B \in \mathcal{M}_{n}$ que verifica $AB = BA = I_n$.\\
Representa-se a inversa de $A$: $A^{-1}$.\\[0.2cm]
Propriedades:
\begin{itemize}
\item Se $\alpha \neq 0$ então $(\alpha A)^{-1} = \alpha^{-1}A^{-1}$
\item Se $A_1, \dots, A_{k}$ forem invertíveis, então $(A_1, \dots, A_k)^{-1} = A_k^{-1} \dots A_1^{-1}$\\
\indent Se $A,B$ forem invertíveis, então $(AB)^{-1} = B^{-1} A^{-1}$
\item Se $A$ invertível, $A^k$ é invertível e $(A^k)^{-1} = (A^-1)^k$.
\end{itemize}
Verificar um dos seguintes verifica todos:
\begin{itemize}
\item $A$ é invertível.
\item $r(A) = m = n$ (o numero de linhas e colunas).
\item $I_n$ é a forma de escada reduzida de $A$.
\item Pode escrever-se $A$ como produto de matrizes elementares.
\end{itemize}
\subsection{Dedução de inversa}
Para se obter a inversa de uma matriz que se sabe ser invertível, considera-se uma matriz identidade $I_n$.\\
Aplica-se a ambas as matrizes as mesmas transformações elementares com o objetivo de tornar a matriz a inverter numa matriz identidade. Quando tal acontecer, a matriz que originalmente era identidade, é agora  a inversa da matriz que se pretendia inverter.
\section{Transposição}
A transposta de uma matriz $A \in \mathcal{M}_{m \times n}$ é a matriz cujos elementos tem a linha trocada com a coluna. $$\forall i,j \in \mathbb{N}, a \in A: (A^T)_{ij} = a_{ji}$$
$A^T$ diz-se a transposta da matriz $A$.
$$
\begin{bmatrix}
\color{red}a_{11} & \color{red}a_{12} & \dots & \color{red}a_{1n}\\
\color{green}a_{21} & \color{green}a_{22} & \dots & \color{green}a_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
\color{blue}a_{m1} & \color{blue}a_{m2} & \dots & \color{blue}a_{mn}
\end{bmatrix}^T
=
\begin{bmatrix}
\color{red}a_{11} & \color{green}a_{21} & \dots & \color{blue}a_{m1}\\
\color{red}a_{12} & \color{green}a_{22} & \dots & \color{blue}a_{m2}\\
\vdots & \vdots & \ddots & \vdots\\
\color{red}a_{1n} & \color{green}a_{2n} & \dots & \color{blue}a_{mn}
\end{bmatrix}
$$
Propriedades:
\begin{itemize}
\item $(A^T)^T = A$
\item $(A+B)^T = A^T + B^T$
\item $(\alpha A)^T = \alpha A^T$
\item $(AB)^T = B^T A^T$
\item $(A^k)^T = (A^T)^k$
\item Se $A$ invertível, então $A^T$ invertível e $(A^T)^{-1} = (A^{-1})^T$
\end{itemize}
Uma matriz igual à transposta é dita \textbf{simétrica}.\\
Se $A=-A^T$ então é dita \textbf{hemi-simétrica}.
\section{Conjugação}
Quando uma matriz $A \in \mathcal{M}$ é composta por elementos de $\mathbb{C}$, a sua conjugada, representada $\overline{A}$, é uma matriz na qual todos os elementos foram substituídos pelo conjugado.\\
Propriedades:
\begin{itemize}
\item $\overline{A+B} = \overline A + \overline B$
\item $\overline{\alpha A} = \overline \alpha \cdot \overline A$
\item $\overline{AC} = \overline A  \> \overline C$
\item $\overline{A^k} = (\overline{A})^k$
\item Se $\exists A^{-1} \Rightarrow  (\overline{A})^{-1} = \overline{A^{-1}}$\\
Para uma matriz invertível, a inversa da conjugada é igual á conjugada da inversa.
\item $\overline{A^T} = (\overline{A})^T$\\
Qualquer destas operações é chamada de \textbf{transconjugação} de $A$, representando-se $A^\star$.
\end{itemize}
A partir da definição de transconjugação (acima) tem-se que:
\begin{itemize}
\item Se $A = A^\star$, (ou seja $a_{ij} = \overline{a}_{ji}$), a matriz é dita \textbf{hermítica}.
\item Se $A = -A^\star$, (ou seja $a_{ij} = -\overline{a}_{ji}$), a matriz é dita \textbf{hemi-hermítica}.
\end{itemize}
\chapter{Sistemas de Equações Lineares}
\section{Definição}
Uma equação linear é qualquer equação que apresente a forma $a_1 x_1 + a_2 x_2 + \dots + a_n x_n = b$, com $a_1, a_2, \dots, a_n \in \mathbb{C}$.
Repare-se que o maior dos graus de uma equação linear é 1. Se apresentasse grau 2 seria equação quadrática, 3 cúbica \dots\\
Relembre-se que a $b$ se dá o nome de \textbf{termo independente}.\\[0.2cm]
Um sistema de equações lineares costuma ser representado da seguinte forma:
$$
\begin{cases}
a_{11} x_1 + \dots + a_{1n} x_n = b_1\\
a_{21} x_1 + \dots + a_{2n} x_n = b_2\\
\dots\\
a_{m1} x_1 + \dots + a_{mn} x_n = b_m\\
\end{cases}
$$
Chama-se a $\beta_1, \beta_2, \dots, \beta_n \in \mathbb{C}$ de \textbf{solução} se a substituição $\forall i, x_i = b_i$ resultar apenas em preposições verdadeiras.
\begin{itemize}
\item A existência de uma única solução faz do sistema \textbf{determinado}.
\item A existência de múltiplas soluções faz do sistema \textbf{indeterminado}.
\item A inexistência de solução faz do sistema \textbf{impossível}.
\end{itemize}
Se dois sistemas são \textbf{equivalentes} se tiverem o mesmo conjunto de soluções.
Define-se ainda que:
\begin{itemize}
\item Se $\exists i : b_i = 0$ então a equação linear na linha $i$ é \textbf{homogénea}.
\item Se $\forall i, b_i = 0$ então o sistema de equações lineares é \textbf{homogéneo}.
\end{itemize}
\section{Representação Matricial}
Um sistema de equações lineares pode ser representado matricialmente por base em 3 componentes:
\begin{enumerate}
\item A matriz dos \textbf{coeficientes}:
$$
A \in \mathcal{M}_{m \times n} =
\begin{bmatrix}
a_{11} & \dots & a_{1n}\\
\vdots & \ddots & \vdots\\
a_{m1} & \dots & a_{mn}\\
\end{bmatrix}
$$
\item A matriz(/vetor) das \textbf{incógnitas}:
$$
X \in \mathcal{M}_{m \times 1} =
\begin{bmatrix}
x_1\\
x_2\\
\vdots \\
x_n
\end{bmatrix}
$$
\item A matriz(/vetor) dos \textbf{termos independentes}:
$$
B \in \mathcal{M}_{m \times 1} =
\begin{bmatrix}
b_1\\
b_2\\
\vdots \\
b_n
\end{bmatrix}
$$
\end{enumerate}
A representação do sistema é então $AX=B$.\\
Efetuar o produto de matrizes torna possível verificar que se trata do mesmo sistema.
\subsection{Matriz Ampliada}
É prático recorrer a uma representação alternativa do sistema, a sua matriz ampliada.\\
Para duas matrizes $A, B$ a sua ampliada é representada por $A|B$ e escrita:
$$
A|B=
  \left(\begin{array}{cccc|c}
    a_{11} & a_{12} & \dots &a_{1n} & b_1 \\
    a_{21} & a_{22} & \dots &a_{2n} & b_2 \\
    \vdots & \vdots & \ddots &\vdots & \vdots \\
    a_{m1} & a_{m2} & \dots &a_{mn} & b_n
  \end{array}\right)
$$
\subsection{Propriedades}
\begin{itemize}
\item Se $P \in \mathcal{M}_{k \times m}$ for invertível então $AX=B \equiv (PA)X=B$ (os sistemas são equivalentes).
\item Transformações por linhas resultam em sistemas equivalentes.
$$\left[A|B\right]{\scriptstyle \overrightarrow{\quad T_l \quad}} \left[A'| B'\right] \quad \rightarrow \quad AX = B \equiv A'X=B'$$
\end{itemize}
\subsection{Determinabilidade}
A caraterística dos coeficientes é sempre menor ou igual á caraterística da matriz ampliada do sistema.
$$r(A) \leq r(A|B)$$
\begin{minipage}{\textwidth}
Fazendo a forma de escada da matriz ampliada é possível saber que:
\begin{itemize}
\item Se a caraterística dos coeficientes for menor que a da matriz ampliada, o sistema é \textbf{impossível}.
$$r(A) < r(A|B)$$
\item Se a caraterística dos coeficientes for igual à da matriz ampliada, o sistema é \textbf{possível}.
$$r(A) = r(A|B)$$
\begin{itemize}
\item Se a caraterística for igual ao numero de variáveis (e de colunas), o sistema é \textbf{possível determinado}.
$$r(A) = r(A|B) = n$$
\item Se a caraterística for menor que o numero de variáveis, o sistema é \textbf{possível indeterminado}.
$$r(A) = r(A|B) < n$$
\item O \textbf{grau de indeterminação} é dado por $n - r(A)$.
\end{itemize}
\item As $r(A)$ incógnitas correspondentes a linhas com pivôs são \textbf{básicas}.
\item As $n - r(A)$ incógnitas correspondentes a linhas sem pivôs são \textbf{livres}.
\end{itemize}
\end{minipage}\\[0.5cm]
{\textbf{Resumindo}:\\
\begin{tikzpicture}
  [
    every node/.style={circle,draw},
    sibling distance=5em,
    grow                    = right,
    level distance          = 19em,
    edge from parent/.style = {draw, -latex},
    every node/.style       = {font=\small},
    sloped
  ]
  \node [root] {Sistema\\$AX = B$}
    child { node [env] {Sistema impossível}
      edge from parent node [below] {$r(A) < r([A|B])$} }
    child { node [env] {Sistema possível}
      child { node [env] {Sistema possível\\ determinado}
        edge from parent node [below] {$r(A) = r([A|B])$} }
      child { node [env] {Sistema possível indeterminado\\ Grau de indeterminação $n - r(A)$\\}
        edge from parent node [above] {$r(A) < r([A|B])$} }};
\end{tikzpicture}}
\section{Sistemas de Cramer}
Um sistema $AX = B$ diz-se sistema de Cramer se $A$ é invertível.\\
Obtendo $A^{-1}$ consegue-se resolver o sistema.
$$A^{-1}\left(A \begin{bmatrix} a_1 \\ \vdots \\ a_n \end{bmatrix} \right) = A^{-1}B
\quad \Leftrightarrow \quad
\left(A^{-1}A\right) \begin{bmatrix} a_1 \\ \vdots \\ a_n \end{bmatrix} = A^{-1}B
\quad \Leftrightarrow \quad
I_n \begin{bmatrix} a_1 \\ \vdots \\ a_n \end{bmatrix} = A^{-1}B
\quad \Leftrightarrow \quad
\begin{bmatrix} a_1 \\ \vdots \\ a_n \end{bmatrix} = A^{-1}B
$$
\subsection{Regra de Cramer}
(Por fazer)
\section{Determinantes}
O determinante é um valor escalar que se obtém a partir de matrizes e permite inferir algumas propriedades das mesmas. É um valor abstrato, mas pode ser interpretado como a magnitude das alterações que a matriz implica.\\
O determinante de uma matriz $A$ representa-se $det A$ ou $|A|$ e em matrizes $\mathcal{M}_{1 \times 1}$ (ou seja $[k]$) o determinante é o próprio elemento ($k$). Para matrizes maiores será visto de seguida como calcular.
\subsection{Complemento Algébrico}
\textbf{Notação}: $A_{(i|j)} \in \mathcal{M}_{(m-1) \times (n-1)}$ é a matriz que se obtém a partir de $A \in \mathcal{M}_{m \times n}$ removendo a linha $i$ e a coluna $j$.\\[0.5cm]
Se $a_{ij}$ for um elemento de uma matriz, o seu \textbf{complemento algébrico} é representado $\hat a_{i,j}$.\\
O complemento algébrico serve principalmente para calcular determinantes de matrizes. É definido:
$$\hat a_{ij} = (-1)^{i+j} det A(i|j)$$
Pode pensar-se que a matriz é um tabuleiro de xadrez com casas $1$ e $-1$, em que $a_{11}$ é uma casa $1$.\\
Seja o seguinte exemplo uma $\mathcal{M}_{4 \times 4}$, em que as casas brancas são multiplicadas por 1 e as pretas por $-1$:
\begin{center}
\begin{tikzpicture}[x=1cm]
\foreach \y in {0,2,...,3}{
    \foreach \x in {0,2,...,3}{
        \fill (\x,\y) rectangle (1+\x,1+\y) rectangle (2+\x,2+\y);}}
\end{tikzpicture}
\end{center}
O complemento algébrico de cada elemento da matriz é o valor da sua "casa de xadrez" a multiplicar pelo determinante da matriz sem essa linha e coluna.\\[0.2cm]
A \textbf{matriz dos complementos algébricos} é a matriz em que todos os elementos são complementos algébricos de outra matriz, nas respetivas posições.
\subsection{Teorema de Laplace}
O método genérico para a obtenção de determinantes é pelo teorema de Laplace, que indica o seguinte:
\begin{enumerate}
\item De uma matriz $A \in \mathcal{M}_{n \times n}$ escolha-se uma linha ou coluna, preferencialmente a que tenha mais zeros.
\begin{center}(A título de exemplo escolha-se a linha $l$)\end{center}
\item Somam-se os produtos entre os elementos dessa linha e os seus complementos algébricos.
$$\sum_l \left(a_{lj} \times \hat a_{lj}\right)$$
\item Re-aplica-se este teorema novamente a eventuais sub-matrizes resultantes.\\
\textbf{Nota}: Não aplicar em matrizes $\mathcal{M}_{1\times 1}$, aplicar $det [k] = k$.
\end{enumerate}
\subsection{Regra de Sarrus}
\begin{minipage}{\textwidth}
A regra de Sarrus é uma simplificação do calculo do determinante aplicável só em matrizes $\mathcal{M}_{3\times 3}$.\\
É possível obter a expressão para o determinante de uma matriz $A \in \mathcal{M}_{3 \times 3}$ a partir do teorema de Laplace:
$$a_{11} \times a_{22} \times a_{33} + a_{12} \times a_{23} \times a_{31} + a_{13} \times a_{21} \times a_{32}
 - a_{31} \times a_{22} \times a_{13} - a_{33} \times a_{21} \times a_{12} - a_{32} \times a_{23} \times a_{11}$$
Enquanto que este resultado é abstrato, quando visto geometricamente na matriz torna-se fácil de memorizar:
\begin{itemize}
\item Imagine-se a matriz que resulta da concatenação entre a matriz cujo determinante está a ser calculado, e as suas primeiras duas colunas:
$$
\left(\begin{array}{ccc|cc}
  a_{11} & a_{12} & a_{13} & a_{11} & a_{12} \\
  a_{21} & a_{22} & a_{23} & a_{21} & a_{22} \\
  a_{31} & a_{32} & a_{33} & a_{31} & a_{32}
\end{array}\right)
$$
\item Soma-se o produto dos elementos das diagonais que decrescem:
$$a_{11} \times a_{22} \times a_{33} +$$
$$a_{12} \times a_{23} \times a_{31} +$$
$$a_{13} \times a_{21} \times a_{32} =$$
$$a_{11} \times a_{22} \times a_{33} + a_{12} \times a_{23} \times a_{31} + a_{13} \times a_{21} \times a_{32}$$
\item Subtrai-se o produto dos elementos das diagonais que crescem:
$$- a_{31} \times a_{22} \times a_{13}$$
$$- a_{32} \times a_{23} \times a_{11}$$
$$\quad- a_{33} \times a_{21} \times a_{12}=$$
$$- a_{31} \times a_{22} \times a_{13} - a_{32} \times a_{23} \times a_{11} - a_{33} \times a_{21} \times a_{12}$$
\item Soma-se o resultado dos dois pontos anteriores (com atenção à negatividade).
$$a_{11} \times a_{22} \times a_{33} + a_{12} \times a_{23} \times a_{31} + a_{13} \times a_{21} \times a_{32} - a_{31} \times a_{22} \times a_{13} - a_{32} \times a_{23} \times a_{11} - a_{33} \times a_{21} \times a_{12}$$
\end{itemize}
\begin{center}
Ilustração do o que foi feito:\\
\begin{tikzpicture}[>=stealth]
% Author for this illustration: Thorsten Donig@ https://tex.stackexchange.com/a/32981
  \matrix [%
    matrix of math nodes,
    column sep=1em,
    row sep=1em
  ] (sarrus) {%
    a_{11} & a_{12} & a_{13} & a_{11} & a_{12} \\
    a_{21} & a_{22} & a_{23} & a_{21} & a_{22} \\
    a_{31} & a_{32} & a_{33} & a_{31} & a_{32} \\
  };
  \path ($(sarrus-1-1.north west)-(0.5em,0)$) edge ($(sarrus-3-1.south west)-(0.5em,0)$)
        ($(sarrus-1-3.north east)+(0.5em,0)$) edge ($(sarrus-3-3.south east)+(0.5em,0)$)
        (sarrus-1-1)                          edge            (sarrus-2-2)
        (sarrus-2-2)                          edge[->]        (sarrus-3-3)
        (sarrus-1-2)                          edge            (sarrus-2-3)
        (sarrus-2-3)                          edge[->]        (sarrus-3-4)
        (sarrus-1-3)                          edge            (sarrus-2-4)
        (sarrus-2-4)                          edge[->]        (sarrus-3-5)
        (sarrus-3-1)                          edge[dashed]    (sarrus-2-2)
        (sarrus-2-2)                          edge[->,dashed] (sarrus-1-3)
        (sarrus-3-2)                          edge[dashed]    (sarrus-2-3)
        (sarrus-2-3)                          edge[->,dashed] (sarrus-1-4)
        (sarrus-3-3)                          edge[dashed]    (sarrus-2-4)
        (sarrus-2-4)                          edge[->,dashed] (sarrus-1-5);
  \foreach \c in {1,2,3} {\node[anchor=south] at (sarrus-1-\c.north) {$+$};};
  \foreach \c in {1,2,3} {\node[anchor=north] at (sarrus-3-\c.south) {$-$};};
\end{tikzpicture}
\end{center}
Como esta regra não é útil para matrizes de dimensões diferentes de $3 \times 3$, é preferível utilizar-se exclusivamente o teorema de Laplace. A vantagem desta regra passaria por exemplo por matrizes com dois $0$s em diagonais distintas para simplificar os cálculos, mas em algumas dessas o teorema de Laplace também pode fazer uso desses $0$s.
\end{minipage}
\subsection{Propriedades dos Determinantes}
Sejam $A, A', A'', B \in \mathcal{M}_{n \times n}$ e $\alpha \in \mathbb{C}$:
\begin{itemize}
\item A existência de uma linha ou coluna nula em $A$ implica um determinante nulo.\\
Imagine-se teorema de Laplace aplicado nessa linha. Todos os complementos algébricos multiplicam por 0.
\item Se $A$ tiver duas linhas ou colunas iguais implica um determinante nulo.
\item O determinante de matrizes triangulares é o produto dos elementos da diagonal.
\item Transpor uma matriz não altera o seu determinante. $det A = det A^T$.
\item Se existirem $A', A''$ matrizes baseadas em $A$ com todas as linhas idênticas á exceção da linha $i$, e $\forall j : (A')_{ij} + (A')_{ij} = a_{ij}$ , então a $det A' + det A'' = det A$.
$$
det
\begin{bmatrix}
a_{11} & \dots & a_{1n}\\
\dots & \dots & \dots \\
b_{i1} + c_{i1} & \dots & b_{in} + c_{in}\\
\dots & \dots & \dots \\
a_{11} & \dots & a_{nn}\\
\end{bmatrix}
=
det
\begin{bmatrix}
a_{11} & \dots & a_{1n}\\
\dots & \dots & \dots \\
b_{i1} & \dots & b_{in} \\
\dots & \dots & \dots \\
a_{11} & \dots & a_{nn}\\
\end{bmatrix}
+
det
\begin{bmatrix}
a_{11} & \dots & a_{1n}\\
\dots & \dots & \dots \\
c_{i1} & \dots & c_{in}\\
\dots & \dots & \dots \\
a_{11} & \dots & a_{nn}\\
\end{bmatrix}
$$
\end{itemize}
Com $i,j$ duas linhas (ou colunas) \underline{distintas}, o impacto das operações elementares é:
\begin{enumerate}
\item O determinante após $\overrightarrow{\quad l_i \leftrightarrow l_j \quad}$ é o simétrico:
$$ A {\scriptstyle{\overrightarrow{\quad l_i \leftrightarrow l_j \quad}}} B, \quad det A = - det B$$
\item O determinante após $\overrightarrow{\quad \alpha \times l_i \quad}$ multiplica por $\alpha$:
$$ A {\scriptstyle{\overrightarrow{\quad \alpha \times l_i \quad}}} B, \quad \alpha \times det A = det B$$
\item O determinante após $\overrightarrow{\quad l_i + \alpha l_j \quad}$ mantem-se inalterado:
$$ A {\scriptstyle{\overrightarrow{\quad l_i + \alpha l_j \quad}}} B, \quad  det A = det B$$
\end{enumerate}
A partir do impacto das operações elementares no calculo do determinante pode concluir-se:
\begin{itemize}
\item Por base na segunda alínea, durante o calculo de um determinante as linhas e colunas podem ser postas em evidência:
$$\begin{vmatrix}
2 & 4 & 6 \\
3 & 9 & 15 \\
5 & 0 & 5
\end{vmatrix}
= 2
\begin{vmatrix}
1 & 2 & 3 \\
3 & 9 & 15 \\
5 & 0 & 5
\end{vmatrix}
= 2 \times 3
\begin{vmatrix}
1 & 2 & 3 \\
1 & 3 & 5 \\
5 & 0 & 5
\end{vmatrix}
= 2 \times 3 \times 5
\begin{vmatrix}
1 & 2 & 3 \\
1 & 3 & 5 \\
1 & 0 & 1
\end{vmatrix}
$$
\item $det(\alpha A) = \alpha^n det A$ (com $n$ o lado da matriz) visto que todas as linhas/colunas podem ter $\alpha$ em evidência.
\item Determinantes nulos após operações elementares implicam que a matriz original tinha determinante nulo.
\item A matriz $A$ só é invertível se $det A \neq 0$.\\
Ser invertível implica $r(A) = n$, o que implica que existe uma matriz triangular superior equivalente a $A$.\\
Em triangulares superiores o calculo do determinante é exequível com o produto dos elementos da diagonal.\\
Para o determinante ser nulo um ou mais desses elementos tem de ser nulos, o que implica $r(A) \neq n$.
\end{itemize}
O determinante do produto de duas matrizes é o produto do determinante dessas duas matrizes:
$$det(AB) = det(A)det(B)$$
O determinante da inversa é o inverso do determinante (relembrar: determinante nulo implica que não existe inversa):
$$det A^{-1} = \frac{1}{det A}$$
\section{Matriz adjunta}
A matriz adjunta, representada $adj A$ é dada pela transporta da matriz dos complementos algébricos.
$$adj A = \hat A^T$$
A partir da adjunta é possível obter a inversa:
$$A^-1 = \frac{1}{det A} adj A$$
A adjunta verifica ainda a seguinte propriedade:
$$
A \times adj A =
\begin{bmatrix}
det A & 0 & 0\\
0 & \ddots & 0\\
0 & 0 & det A
\end{bmatrix}
$$
\end{document}