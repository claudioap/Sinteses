\documentclass[]{report}
\usepackage[a4paper, total={7in, 8.5in}]{geometry}
\usepackage[portuguese]{babel}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage[dvipsnames]{xcolor}
\usepackage{forest}
\usepackage{tikz}
\usetikzlibrary{calc,matrix}
\tikzset{
  treenode/.style = {align=center},
  root/.style     = {treenode, font=\Large},
  env/.style      = {treenode, font=\normalsize},
  dummy/.style    = {circle,draw}
}

\begin{document}
\begin{titlepage}
	\centering
	\vspace{5cm}
	{\huge\bfseries Álgebra Linear e Geometria Analítica\par}
	\vspace{1cm}
	{\scshape\Large Síntese baseada no conteúdo lecionado na\\
	 FCT/Universidade Nova de Lisboa\par}
	\vspace{2cm}
	Adaptado por:\\
	{\Large \textit{Cláudio Afonso de Sousa Pereira}\\
	(sinteses$\text{@}$claudiop$.$com)\par}
	\vspace{1cm}
	{\large \today\par}
	\vfill
	Adaptação licenciada:\\
	\href{http://creativecommons.org/licenses/by-sa/4.0/}{\includegraphics[scale=0.8]{../ccbysa.png}}
\end{titlepage}
\tableofcontents
\chapter{Calculo Matricial}
\section{Matrizes}
Matrizes são estruturas compostas por diversos números em grelha, tendo propriedades e operações próprias. São úteis a certas áreas de estudo podendo ajudar a abstrair problemas.\\
Uma matriz $m$ por $n$ (interpretada "matriz com $m$ linhas e $n$ colunas") tem o formato:
$$\begin{bmatrix}
a_{11} & a_{12} & \dots & a_{1n} \\
a_{21} & a_{22} & \dots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \dots & a_{mn}
\end{bmatrix}$$
Assim sendo, $a_{ij}$ é dito elemento na linha $i$ e coluna $j$.\\
Os elementos pode pertencer aos reais ($\mathbb{R}$) ou aos complexos ($\mathbb{C}$).\\
Se $\forall i,j : a_{ij} \in \mathbb{R}$ a matriz acima é declarada $\mathcal{M}_{m \times n} \in \mathbb{R}$.
\subsection{Matrizes notáveis}
Alguns formatos matrizes são especialmente importantes:
\begin{itemize}
\item \textbf{Quadrada} - Toda a que tenha o mesmo numero de linhas e colunas.\\
$\mathcal{M}_{n \times n}$ - Quadrada da ordem $n$, abreviada $\mathcal{M}_{n}$.\\
A \textbf{diagonal principal} de uma matriz são os elementos entre o canto superior esquerdo e o inferior direito.
\item \textbf{Triangular} - Quadrada nulificada de um dos lados da diagonal principal.
$$\begin{bmatrix}
a_{11} & \dots & a_{1n} \\
0 & \ddots & \vdots \\
0 & 0 & a_{mn}
\end{bmatrix}$$
\item \textbf{Diagonal} - Quadrada que só tem elementos não-nulos na diagonal principal.
$$\begin{bmatrix}
a_{11} & 0 & 0 \\
0 & \ddots & 0\\
0 & 0 & a_{mn}
\end{bmatrix}$$
\item \textbf{Escalar} - Diagonal em que todos os elementos da diagonal principal tem o mesmo valor.
$$\begin{bmatrix}
k & 0 & 0 \\
0 & \ddots & 0\\
0 & 0 & k
\end{bmatrix}$$
\item \textbf{Identidade} - Quadrada com os elementos da diagonal principal $=1$.
$$\begin{bmatrix}
1 & 0 & 0 \\
0 & \ddots & 0\\
0 & 0 & 1
\end{bmatrix}$$
\item \textbf{Nula} - Constituída por elementos nulos.
$$\begin{bmatrix}
0 & \dots & 0 \\
\vdots & \ddots & \vdots\\
0 & \dots & 1
\end{bmatrix}$$
\item \textbf{Linha} - Apenas tem uma linha.
$$\begin{bmatrix}
a_{11} & \dots & a_{1n}
\end{bmatrix}$$
\item \textbf{Coluna} - Apenas tem uma coluna.
$$\begin{bmatrix}
a_{1} \\
\vdots \\
a_{m1}
\end{bmatrix}$$
\end{itemize} 
\section{Operações aritméticas}
\subsection{Adição}
Duas matrizes $A$ e $B$ podem somadas da seguinte forma:
$$A+B = 
\begin{bmatrix}
\color{red}a_{11} & \color{red}a_{12} & \dots & \color{red}a_{1n} \\
\color{red}a_{21} & \color{red}a_{22} & \dots & \color{red}a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
\color{red}a_{m1} & \color{red}a_{m2} & \dots & \color{red}a_{mn}
\end{bmatrix}
+
\begin{bmatrix}
\color{blue}b_{11} & \color{blue}b_{12} & \dots & \color{blue}b_{1n} \\
\color{blue}b_{21} & \color{blue}b_{22} & \dots & \color{blue}b_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
\color{blue}b_{m1} & \color{blue}b_{m2} & \dots & \color{blue}b_{mn}
\end{bmatrix}
=
\begin{bmatrix}
\textcolor{red}{a_{11}}+\textcolor{blue}{b_{11}} & \textcolor{red}{a_{12}}+\textcolor{blue}{b_{12}} & \dots & \textcolor{red}{a_{1n}}+\textcolor{blue}{b_{1n}} \\
\textcolor{red}{a_{21}}+\textcolor{blue}{b_{21}} & \textcolor{red}{a_{22}}+\textcolor{blue}{b_{22}} & \dots & \textcolor{red}{a_{2n}}+\textcolor{blue}{b_{2n}} \\
\vdots & \vdots & \ddots & \vdots \\
\textcolor{red}{a_{m1}}+\textcolor{blue}{b_{m1}} & \textcolor{red}{a_{m2}}+\textcolor{blue}{b_{m2}} & \dots & \textcolor{red}{a_{mn}}+\textcolor{blue}{b_{mn}}
\end{bmatrix}$$
A adição de matrizes é comutativa ($A+B = B+A$) e associativa ($A+(B+C) = (A+B)+C$).
\subsection{Produto escalar}
$$
\textcolor{red}{\alpha}
\begin{bmatrix}
\color{blue}b_{11} & \color{blue}b_{12} & \dots & \color{blue}b_{1n} \\
\color{blue}b_{21} & \color{blue}b_{22} & \dots & \color{blue}b_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
\color{blue}b_{m1} & \color{blue}b_{m2} & \dots & \color{blue}b_{mn}
\end{bmatrix}
=
\begin{bmatrix}
\textcolor{red}{\alpha}+\textcolor{blue}{b_{11}} & \textcolor{red}{\alpha}+\textcolor{blue}{b_{12}} & \dots & \textcolor{red}{\alpha}+\textcolor{blue}{b_{1n}} \\
\textcolor{red}{\alpha}+\textcolor{blue}{b_{21}} & \textcolor{red}{\alpha}+\textcolor{blue}{b_{22}} & \dots & \textcolor{red}{\alpha}+\textcolor{blue}{b_{2n}} \\
\vdots & \vdots & \ddots & \vdots \\
\textcolor{red}{\alpha}+\textcolor{blue}{b_{m1}} & \textcolor{red}{\alpha}+\textcolor{blue}{b_{m2}} & \dots & \textcolor{red}{\alpha}+\textcolor{blue}{b_{mn}}
\end{bmatrix}$$
\begin{itemize}
\item $\alpha(A+B) = \alpha A + \alpha B$
\item $(\alpha + \beta)A = \alpha A + \beta A$
\item $(\alpha\beta)A = \alpha(\beta A)$
\end{itemize}
\subsection{Produto}
O produto de matrizes requer que a esquerda seja $\mathcal{M}_{m\times k}$ e a direita $\mathcal{M}_{k\times n}$.\\
Como o numero de colunas da primeira tem que ser igual ao numero de linhas da segunda intuitivamente o produto de matrizes \underline{não é comutativo} (nem quando ambas são quadradas).
$$(AB)_{ij} = \sum^n_{k=1} a_{ik}b_{kj}$$
Isto é, o elemento $(AB)_{i,j}$ da matriz resultante é dado por:
$$AB = 
\begin{bmatrix}
\vdots & \vdots & \vdots & \vdots \\
\color{red}a_{i1} & \color{red}a_{i2} & \dots & \color{red}a_{in} \\
\vdots & \vdots & \vdots & \vdots
\end{bmatrix}
\times
\begin{bmatrix}
\dots & \color{blue}b_{12} & \dots \\
\dots & \color{blue}b_{22} & \dots\\
\vdots & \vdots & \vdots \\
\dots & \color{blue}b_{mj} & \dots
\end{bmatrix}
=
\begin{bmatrix}
\dots & \dots & \dots\\
\dots & \textcolor{red}{a_{i1}}\textcolor{blue}{b_{1j}} + \textcolor{red}{a_{i2}}\textcolor{blue}{b_{2j}} + \dots + \textcolor{red}{a_{in}}\textcolor{blue}{b_{nj}} & \dots\\
\dots & \dots & \dots
\end{bmatrix}$$
\begin{itemize}
\item $(AB)C = A(BC)$ (Associativa)
\item $A(B+C) = AB + AC, \quad (B+C)A = BA + CA$ (Distributiva, á esquerda e direita)
\item $\alpha(AB) = (\alpha A)B = A(\alpha B)$ (Multiplicação por escalar)
\item $AI_n = I_m A = A$ (Multiplicação por identidade)
\item Se $AB=AC$ ou $BA = CA$, sabendo que $A \neq 0$, sabe-se que $B=C$.
\end{itemize}
\subsection{Potência}
$A^k
\begin{cases}
I_n&,\text{se } k=1\\
A^{k-1}A&,\text{se } k=\mathbb{N}
\end{cases}
$\\
$A^k A^l = A^{k+l}$\\
$A^{k^l} = A^{kl}$
\section{Transformações Elementares}
A partir das operações aritméticas em matrizes, há operações compostas que são definidas como transformações elementares por linhas (ou por colunas). São as seguintes (quando por linhas):
\begin{enumerate}
\item \textbf{Troca de linhas}
$$
\begin{bmatrix}
a_{11} & a_{12} & \dots & a_{1n}\\
\dots & \dots & \dots & \dots\\
\color{red}a_{i1} & \color{red}a_{i2} & \dots & \color{red}a_{in}\\
\dots & \dots & \dots & \dots\\
\color{blue}a_{j1} & \color{blue}a_{j2} & \dots & \color{blue}a_{jn}\\
\dots & \dots & \dots & \dots\\
a_{11} & a_{12} & \dots & a_{1n}\\
\end{bmatrix}
\overrightarrow{l_i \leftrightarrow l_j}
\begin{bmatrix}
a_{11} & a_{12} & \dots & a_{1n}\\
\dots & \dots & \dots & \dots\\
\color{blue}a_{j1} & \color{blue}a_{j2} & \dots & \color{blue}a_{jn}\\
\dots & \dots & \dots & \dots\\
\color{red}a_{i1} & \color{red}a_{i2} & \dots & \color{red}a_{in}\\
\dots & \dots & \dots & \dots\\
a_{11} & a_{12} & \dots & a_{1n}\\
\end{bmatrix}
$$
\item \textbf{Multiplicação a linha}
$$
\begin{bmatrix}
a_{11} & a_{12} & \dots & a_{1n}\\
\dots & \dots & \dots & \dots\\
\color{red}a_{i1} & \color{red}a_{i2} & \dots & \color{red}a_{in}\\
\dots & \dots & \dots & \dots\\
a_{11} & a_{12} & \dots & a_{1n}\\
\end{bmatrix}
\overrightarrow{\textcolor{blue}{\alpha} l_i}
\begin{bmatrix}
a_{11} & a_{12} & \dots & a_{1n}\\
\dots & \dots & \dots & \dots\\
\color{blue}\alpha\color{red}a_{i1} & \color{blue}\alpha\color{red}a_{i2} & \dots & \color{blue}\alpha\color{red}a_{in}\\
\dots & \dots & \dots & \dots\\
a_{11} & a_{12} & \dots & a_{1n}\\
\end{bmatrix}
$$
\textbf{Nota}: é necessário garantir que $\alpha \neq 0$.
\item \textbf{Soma com múltiplo de outra linha}
$$
\begin{bmatrix}
a_{11} & a_{12} & \dots & a_{1n}\\
\dots & \dots & \dots & \dots\\
\color{red}a_{i1} & \color{red}a_{i2} & \dots & \color{red}a_{in}\\
\dots & \dots & \dots & \dots\\
\color{blue}a_{j1} & \color{blue}a_{j2} & \dots & \color{blue}a_{jn}\\
\dots & \dots & \dots & \dots\\
a_{11} & a_{12} & \dots & a_{1n}\\
\end{bmatrix}
\overrightarrow{l_i + \textcolor{green}{\beta} l_j}
\begin{bmatrix}
a_{11} & a_{12} & \dots & a_{1n}\\
\dots & \dots & \dots & \dots\\
\textcolor{red}{a_{i1}}+ \color{green}\beta\color{blue}a_{j1} & \textcolor{red}{a_{i2}}+ \color{green}\beta\color{blue}a_{j2} & \dots & \textcolor{red}{a_{in}}+ \color{green}\beta\color{blue}a_{jn}\\
\dots & \dots & \dots & \dots\\
\color{blue}a_{j1} & \color{blue}a_{j2} & \dots & \color{blue}a_{jn}\\
\dots & \dots & \dots & \dots\\
a_{11} & a_{12} & \dots & a_{1n}\\
\end{bmatrix}
$$
\end{enumerate}
Como dito, as mesmas operações podem ser feitas por colunas.\\[0.5cm]
Seja $A \in \mathcal{M}_{m \times n}$ e $\alpha, \beta \in \mathbb{R}$.
\begin{itemize}
\item $A {\scriptstyle{\overrightarrow{\quad T \quad}}} B$ representa que $B$ resulta de uma transformação elementar, do tipo $T$, aplicada a $A$.
\item $A$ é \textbf{equivalente por linhas} a $B$ se um numero finito de transformações elementares por linhas em $A$ resultar em $B$.
\end{itemize}
\subsection{Matrizes elementares e formas de escada}
Á matriz que resulte da aplicação de \underline{uma única} transformação elementar a uma matriz identidade, designa-se de \textbf{matriz elementar}.
\begin{itemize}
\item Uma matriz tem uma transformação por linhas quando pré-multiplicada por uma matriz elementar.
\item Uma matriz tem uma transformação por colunas quando pós-multiplicada por uma matriz elementar.
\item A inversa de uma matriz elementar reverte a operação que a elementar aplica.
\end{itemize}
Seja o elemento \textbf{pivô} de uma linha o elemento não nulo mais à esquerda dessa linha e que não apresenta mais nenhum elemento não nulo por baixo de si (na coluna).
Quando uma matriz apresenta um pivô em todas as linhas não nulas, e quaisquer eventuais linhas nulas venham depois das não nulas, então a matriz tem \textbf{forma de escada}.\\[0.2cm]
O número de linhas não nulas de uma matriz em escada é chamado de \textbf{caraterística}.
A caraterística da matriz $A$ representa-se $r(A)$ e é sempre menor ou igual tanto ao numero de linhas como de colunas.\\[0.2cm]
Verificam-se as seguintes propriedades:
\begin{itemize}
\item Toda a matriz é equivalente por linhas a uma matriz em forma de escada.
\item Se uma matriz  escada contiver 1 como elemento pivô de cada coluna, e todos os restantes elementos dessa coluna forem nulos, a matriz é dita em forma de \textbf{escada reduzida} e é única. 
\end{itemize}
\section{Invertíbilidade}
Sejam $A, B \in \mathcal{M}_{n}$ (matrizes quadradas), $\alpha \in \mathbb{R}$ 
(ou $\mathbb{C}$) e $k \in \mathbb{N}$.\\
A matriz $A \in \mathcal{M}_{n}$ é invertível se existe uma única matriz $B \in \mathcal{M}_{n}$ que verifica $AB = BA = I_n$.\\
Representa-se a inversa de $A$: $A^{-1}$.\\[0.2cm]
Propriedades:
\begin{itemize}
\item Se $\alpha \neq 0$ então $(\alpha A)^{-1} = \alpha^{-1}A^{-1}$
\item Se $A_1, \dots, A_{k}$ forem invertíveis, então $(A_1, \dots, A_k)^{-1} = A_k^{-1} \dots A_1^{-1}$\\
\indent Se $A,B$ forem invertíveis, então $(AB)^{-1} = B^{-1} A^{-1}$
\item Se $A$ invertível, $A^k$ é invertível e $(A^k)^{-1} = (A^-1)^k$.
\end{itemize}
Verificar um dos seguintes verifica todos:
\begin{itemize}
\item $A$ é invertível.
\item $r(A) = m = n$ (o numero de linhas e colunas).
\item $I_n$ é a forma de escada reduzida de $A$.
\item Pode escrever-se $A$ como produto de matrizes elementares.
\end{itemize}
\subsection{Dedução de inversa}
Para se obter a inversa de uma matriz que se sabe ser invertível, considera-se uma matriz identidade $I_n$.\\
Aplica-se a ambas as matrizes as mesmas transformações elementares com o objetivo de tornar a matriz a inverter numa matriz identidade. Quando tal acontecer, a matriz que originalmente era identidade, é agora  a inversa da matriz que se pretendia inverter.
\section{Transposição}
A transposta de uma matriz $A \in \mathcal{M}_{m \times n}$ é a matriz cujos elementos tem a linha trocada com a coluna. $$\forall i,j \in \mathbb{N}, a \in A: (A^T)_{ij} = a_{ji}$$
$A^T$ diz-se a transposta da matriz $A$.
$$
\begin{bmatrix}
\color{red}a_{11} & \color{red}a_{12} & \dots & \color{red}a_{1n}\\
\color{green}a_{21} & \color{green}a_{22} & \dots & \color{green}a_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
\color{blue}a_{m1} & \color{blue}a_{m2} & \dots & \color{blue}a_{mn}
\end{bmatrix}^T
=
\begin{bmatrix}
\color{red}a_{11} & \color{green}a_{21} & \dots & \color{blue}a_{m1}\\
\color{red}a_{12} & \color{green}a_{22} & \dots & \color{blue}a_{m2}\\
\vdots & \vdots & \ddots & \vdots\\
\color{red}a_{1n} & \color{green}a_{2n} & \dots & \color{blue}a_{mn}
\end{bmatrix}
$$
Propriedades:
\begin{itemize}
\item $(A^T)^T = A$
\item $(A+B)^T = A^T + B^T$
\item $(\alpha A)^T = \alpha A^T$
\item $(AB)^T = B^T A^T$
\item $(A^k)^T = (A^T)^k$
\item Se $A$ invertível, então $A^T$ invertível e $(A^T)^{-1} = (A^{-1})^T$
\end{itemize}
Uma matriz igual à transposta é dita \textbf{simétrica}.\\
Se $A=-A^T$ então é dita \textbf{hemi-simétrica}.
\section{Conjugação}
Quando uma matriz $A \in \mathcal{M}$ é composta por elementos de $\mathbb{C}$, a sua conjugada, representada $\overline{A}$, é uma matriz na qual todos os elementos foram substituídos pelo conjugado.\\
Propriedades:
\begin{itemize}
\item $\overline{A+B} = \overline A + \overline B$
\item $\overline{\alpha A} = \overline \alpha \cdot \overline A$
\item $\overline{AC} = \overline A  \> \overline C$
\item $\overline{A^k} = (\overline{A})^k$
\item Se $\exists A^{-1} \Rightarrow  (\overline{A})^{-1} = \overline{A^{-1}}$\\
Para uma matriz invertível, a inversa da conjugada é igual á conjugada da inversa.
\item $\overline{A^T} = (\overline{A})^T$\\
Qualquer destas operações é chamada de \textbf{transconjugação} de $A$, representando-se $A^\star$.
\end{itemize}
A partir da definição de transconjugação (acima) tem-se que:
\begin{itemize}
\item Se $A = A^\star$, (ou seja $a_{ij} = \overline{a}_{ji}$), a matriz é dita \textbf{hermítica}.
\item Se $A = -A^\star$, (ou seja $a_{ij} = -\overline{a}_{ji}$), a matriz é dita \textbf{hemi-hermítica}.
\end{itemize}
\chapter{Sistemas de Equações Lineares}
\section{Definição}
Uma equação linear é qualquer equação que apresente a forma $a_1 x_1 + a_2 x_2 + \dots + a_n x_n = b$, com $a_1, a_2, \dots, a_n \in \mathbb{C}$.
Repare-se que o maior dos graus de uma equação linear é 1. Se apresentasse grau 2 seria equação quadrática, 3 cúbica \dots\\
Relembre-se que a $b$ se dá o nome de \textbf{termo independente}.\\[0.2cm]
Um sistema de equações lineares costuma ser representado da seguinte forma:
$$
\begin{cases}
a_{11} x_1 + \dots + a_{1n} x_n = b_1\\
a_{21} x_1 + \dots + a_{2n} x_n = b_2\\
\dots\\
a_{m1} x_1 + \dots + a_{mn} x_n = b_m\\
\end{cases}
$$
Chama-se a $\beta_1, \beta_2, \dots, \beta_n \in \mathbb{C}$ de \textbf{solução} se a substituição $\forall i, x_i = b_i$ resultar apenas em preposições verdadeiras.
\begin{itemize}
\item A existência de uma única solução faz do sistema \textbf{determinado}.
\item A existência de múltiplas soluções faz do sistema \textbf{indeterminado}.
\item A inexistência de solução faz do sistema \textbf{impossível}.
\end{itemize}
Se dois sistemas são \textbf{equivalentes} se tiverem o mesmo conjunto de soluções.
Define-se ainda que:
\begin{itemize}
\item Se $\exists i : b_i = 0$ então a equação linear na linha $i$ é \textbf{homogénea}.
\item Se $\forall i, b_i = 0$ então o sistema de equações lineares é \textbf{homogéneo}.
\end{itemize}
\section{Representação Matricial}
Um sistema de equações lineares pode ser representado matricialmente por base em 3 componentes:
\begin{enumerate}
\item A matriz dos \textbf{coeficientes}:
$$
A \in \mathcal{M}_{m \times n} =
\begin{bmatrix}
a_{11} & \dots & a_{1n}\\
\vdots & \ddots & \vdots\\
a_{m1} & \dots & a_{mn}\\
\end{bmatrix}
$$
\item A matriz(/vetor) das \textbf{incógnitas}:
$$
X \in \mathcal{M}_{m \times 1} =
\begin{bmatrix}
x_1\\
x_2\\
\vdots \\
x_n
\end{bmatrix}
$$
\item A matriz(/vetor) dos \textbf{termos independentes}:
$$
B \in \mathcal{M}_{m \times 1} =
\begin{bmatrix}
b_1\\
b_2\\
\vdots \\
b_n
\end{bmatrix}
$$
\end{enumerate}
A representação do sistema é então $AX=B$.\\
Efetuar o produto de matrizes torna possível verificar que se trata do mesmo sistema.
\subsection{Matriz Ampliada}
É prático recorrer a uma representação alternativa do sistema, a sua matriz ampliada.\\
Para duas matrizes $A, B$ a sua ampliada é representada por $A|B$ e escrita:
$$
A|B=
  \left(\begin{array}{cccc|c}
    a_{11} & a_{12} & \dots &a_{1n} & b_1 \\
    a_{21} & a_{22} & \dots &a_{2n} & b_2 \\
    \vdots & \vdots & \ddots &\vdots & \vdots \\
    a_{m1} & a_{m2} & \dots &a_{mn} & b_n
  \end{array}\right)
$$
\subsection{Propriedades}
\begin{itemize}
\item Se $P \in \mathcal{M}_{k \times m}$ for invertível então $AX=B \equiv (PA)X=B$ (os sistemas são equivalentes).
\item Transformações por linhas resultam em sistemas equivalentes.
$$\left[A|B\right]{\scriptstyle \overrightarrow{\quad T_l \quad}} \left[A'| B'\right] \quad \rightarrow \quad AX = B \equiv A'X=B'$$
\end{itemize}
\subsection{Determinabilidade}
A caraterística dos coeficientes é sempre menor ou igual á caraterística da matriz ampliada do sistema.
$$r(A) \leq r(A|B)$$
\begin{minipage}{\textwidth}
Fazendo a forma de escada da matriz ampliada é possível saber que:
\begin{itemize}
\item Se a caraterística dos coeficientes for menor que a da matriz ampliada, o sistema é \textbf{impossível}.
$$r(A) < r(A|B)$$
\item Se a caraterística dos coeficientes for igual à da matriz ampliada, o sistema é \textbf{possível}.
$$r(A) = r(A|B)$$
\begin{itemize}
\item Se a caraterística for igual ao numero de variáveis (e de colunas), o sistema é \textbf{possível determinado}.
$$r(A) = r(A|B) = n$$
\item Se a caraterística for menor que o numero de variáveis, o sistema é \textbf{possível indeterminado}.
$$r(A) = r(A|B) < n$$
\item O \textbf{grau de indeterminação} é dado por $n - r(A)$.
\end{itemize}
\item As $r(A)$ incógnitas correspondentes a linhas com pivôs são \textbf{básicas}.
\item As $n - r(A)$ incógnitas correspondentes a linhas sem pivôs são \textbf{livres}.
\end{itemize}
\end{minipage}\\[0.5cm]
{\textbf{Resumindo}:\\
\begin{tikzpicture}
  [
    every node/.style={circle,draw},
    sibling distance=5em,
    grow                    = right,
    level distance          = 19em,
    edge from parent/.style = {draw, -latex},
    every node/.style       = {font=\small},
    sloped
  ]
  \node [root] {Sistema\\$AX = B$}
    child { node [env] {Sistema impossível}
      edge from parent node [below] {$r(A) < r([A|B])$} }
    child { node [env] {Sistema possível}
      child { node [env] {Sistema possível\\ determinado}
        edge from parent node [below] {$r(A) = r([A|B])$} }
      child { node [env] {Sistema possível indeterminado\\ Grau de indeterminação $n - r(A)$\\}
        edge from parent node [above] {$r(A) < r([A|B])$} }};
\end{tikzpicture}}
\section{Sistemas de Cramer}
Um sistema $AX = B$ diz-se sistema de Cramer se $A$ é invertível.\\
Obtendo $A^{-1}$ consegue-se resolver o sistema.
$$A^{-1}\left(A \begin{bmatrix} a_1 \\ \vdots \\ a_n \end{bmatrix} \right) = A^{-1}B
\quad \Leftrightarrow \quad
\left(A^{-1}A\right) \begin{bmatrix} a_1 \\ \vdots \\ a_n \end{bmatrix} = A^{-1}B
\quad \Leftrightarrow \quad
I_n \begin{bmatrix} a_1 \\ \vdots \\ a_n \end{bmatrix} = A^{-1}B
\quad \Leftrightarrow \quad
\begin{bmatrix} a_1 \\ \vdots \\ a_n \end{bmatrix} = A^{-1}B
$$
\section{Determinantes}
O determinante é um valor escalar que se obtém a partir de matrizes e permite inferir algumas propriedades das mesmas. É um valor abstrato, mas pode ser interpretado como a magnitude das alterações que a matriz implica.\\
O determinante de uma matriz $A$ representa-se $det A$ ou $|A|$ e em matrizes $\mathcal{M}_{1 \times 1}$ (ou seja $[k]$) o determinante é o próprio elemento ($k$). Para matrizes maiores será visto de seguida como calcular.
\subsection{Complemento Algébrico}
\textbf{Notação}: $A_{(i|j)} \in \mathcal{M}_{(m-1) \times (n-1)}$ é a matriz que se obtém a partir de $A \in \mathcal{M}_{m \times n}$ removendo a linha $i$ e a coluna $j$.\\[0.5cm]
Se $a_{ij}$ for um elemento de uma matriz, o seu \textbf{complemento algébrico} é representado $\hat a_{i,j}$.\\
O complemento algébrico serve principalmente para calcular determinantes de matrizes. É definido:
$$\hat a_{ij} = (-1)^{i+j} det A(i|j)$$
Pode pensar-se que a matriz é um tabuleiro de xadrez com casas $1$ e $-1$, em que $a_{11}$ é uma casa $1$.\\
Seja o seguinte exemplo uma $\mathcal{M}_{4 \times 4}$, em que as casas brancas são multiplicadas por 1 e as pretas por $-1$:
\begin{center}
\begin{tikzpicture}[x=1cm]
\foreach \y in {0,2,...,3}{
    \foreach \x in {0,2,...,3}{
        \fill (\x,\y) rectangle (1+\x,1+\y) rectangle (2+\x,2+\y);}}
\end{tikzpicture}
\end{center}
O complemento algébrico de cada elemento da matriz é o valor da sua "casa de xadrez" a multiplicar pelo determinante da matriz sem essa linha e coluna.\\[0.2cm]
A \textbf{matriz dos complementos algébricos} é a matriz em que todos os elementos são complementos algébricos de outra matriz, nas respetivas posições.
\subsection{Teorema de Laplace}
O método genérico para a obtenção de determinantes é pelo teorema de Laplace, que indica o seguinte:
\begin{enumerate}
\item De uma matriz $A \in \mathcal{M}_{n \times n}$ escolha-se uma linha ou coluna, preferencialmente a que tenha mais zeros.
\begin{center}(A título de exemplo escolha-se a linha $l$)\end{center}
\item Somam-se os produtos entre os elementos dessa linha e os seus complementos algébricos.
$$\sum_l \left(a_{lj} \times \hat a_{lj}\right)$$
\item Re-aplica-se este teorema novamente a eventuais sub-matrizes resultantes.\\
\textbf{Nota}: Não aplicar em matrizes $\mathcal{M}_{1\times 1}$, aplicar $det [k] = k$.
\end{enumerate}
\subsection{Regra de Sarrus}
\begin{minipage}{\textwidth}
A regra de Sarrus é uma simplificação do calculo do determinante aplicável só em matrizes $\mathcal{M}_{3\times 3}$.\\
É possível obter a expressão para o determinante de uma matriz $A \in \mathcal{M}_{3 \times 3}$ a partir do teorema de Laplace:
$$a_{11} \times a_{22} \times a_{33} + a_{12} \times a_{23} \times a_{31} + a_{13} \times a_{21} \times a_{32}
 - a_{31} \times a_{22} \times a_{13} - a_{33} \times a_{21} \times a_{12} - a_{32} \times a_{23} \times a_{11}$$
Enquanto que este resultado é abstrato, quando visto geometricamente na matriz torna-se fácil de memorizar:
\begin{itemize}
\item Imagine-se a matriz que resulta da concatenação entre a matriz cujo determinante está a ser calculado, e as suas primeiras duas colunas:
$$
\left(\begin{array}{ccc|cc}
  a_{11} & a_{12} & a_{13} & a_{11} & a_{12} \\
  a_{21} & a_{22} & a_{23} & a_{21} & a_{22} \\
  a_{31} & a_{32} & a_{33} & a_{31} & a_{32}
\end{array}\right)
$$
\item Soma-se o produto dos elementos das diagonais que decrescem ($\searrow$):
$$a_{11} \times a_{22} \times a_{33} +$$
$$a_{12} \times a_{23} \times a_{31} +$$
$$a_{13} \times a_{21} \times a_{32} =$$
$$a_{11} \times a_{22} \times a_{33} + a_{12} \times a_{23} \times a_{31} + a_{13} \times a_{21} \times a_{32}$$
\item Subtrai-se o produto dos elementos das diagonais que crescem ($\nearrow$):
$$- a_{31} \times a_{22} \times a_{13}$$
$$- a_{32} \times a_{23} \times a_{11}$$
$$\quad- a_{33} \times a_{21} \times a_{12}=$$
$$- a_{31} \times a_{22} \times a_{13} - a_{32} \times a_{23} \times a_{11} - a_{33} \times a_{21} \times a_{12}$$
\item Soma-se o resultado dos dois pontos anteriores (com atenção à negatividade).
$$a_{11} \times a_{22} \times a_{33} + a_{12} \times a_{23} \times a_{31} + a_{13} \times a_{21} \times a_{32} - a_{31} \times a_{22} \times a_{13} - a_{32} \times a_{23} \times a_{11} - a_{33} \times a_{21} \times a_{12}$$
\end{itemize}
\begin{center}
Ilustração do o que foi feito:\\
\begin{tikzpicture}[>=stealth]
% Author for this illustration: Thorsten Donig@ https://tex.stackexchange.com/a/32981
  \matrix [%
    matrix of math nodes,
    column sep=1em,
    row sep=1em
  ] (sarrus) {%
    a_{11} & a_{12} & a_{13} & a_{11} & a_{12} \\
    a_{21} & a_{22} & a_{23} & a_{21} & a_{22} \\
    a_{31} & a_{32} & a_{33} & a_{31} & a_{32} \\
  };
  \path ($(sarrus-1-1.north west)-(0.5em,0)$) edge ($(sarrus-3-1.south west)-(0.5em,0)$)
        ($(sarrus-1-3.north east)+(0.5em,0)$) edge ($(sarrus-3-3.south east)+(0.5em,0)$)
        (sarrus-1-1)                          edge            (sarrus-2-2)
        (sarrus-2-2)                          edge[->]        (sarrus-3-3)
        (sarrus-1-2)                          edge            (sarrus-2-3)
        (sarrus-2-3)                          edge[->]        (sarrus-3-4)
        (sarrus-1-3)                          edge            (sarrus-2-4)
        (sarrus-2-4)                          edge[->]        (sarrus-3-5)
        (sarrus-3-1)                          edge[dashed]    (sarrus-2-2)
        (sarrus-2-2)                          edge[->,dashed] (sarrus-1-3)
        (sarrus-3-2)                          edge[dashed]    (sarrus-2-3)
        (sarrus-2-3)                          edge[->,dashed] (sarrus-1-4)
        (sarrus-3-3)                          edge[dashed]    (sarrus-2-4)
        (sarrus-2-4)                          edge[->,dashed] (sarrus-1-5);
  \foreach \c in {1,2,3} {\node[anchor=south] at (sarrus-1-\c.north) {$+$};};
  \foreach \c in {1,2,3} {\node[anchor=north] at (sarrus-3-\c.south) {$-$};};
\end{tikzpicture}
\end{center}
Como esta regra não é útil para matrizes de dimensões diferentes de $3 \times 3$, é preferível utilizar-se exclusivamente o teorema de Laplace. A vantagem desta regra passaria por exemplo por matrizes com dois $0$s em diagonais distintas para simplificar os cálculos, mas em algumas dessas o teorema de Laplace também pode fazer uso desses $0$s.
\end{minipage}
\subsection{Propriedades dos Determinantes}
Sejam $A, A', A'', B \in \mathcal{M}_{n \times n}$ e $\alpha \in \mathbb{C}$:
\begin{itemize}
\item A existência de uma linha ou coluna nula em $A$ implica um determinante nulo.\\
Imagine-se teorema de Laplace aplicado nessa linha. Todos os complementos algébricos multiplicam por 0.
\item Se $A$ tiver duas linhas ou colunas iguais implica um determinante nulo.
\item O determinante de matrizes triangulares é o produto dos elementos da diagonal.
\item Transpor uma matriz não altera o seu determinante. $det A = det A^T$.
\item Se existirem $A', A''$ matrizes baseadas em $A$ com todas as linhas idênticas á exceção da linha $i$, e $\forall j : (A')_{ij} + (A')_{ij} = a_{ij}$ , então a $det A' + det A'' = det A$.
$$
det
\begin{bmatrix}
a_{11} & \dots & a_{1n}\\
\dots & \dots & \dots \\
b_{i1} + c_{i1} & \dots & b_{in} + c_{in}\\
\dots & \dots & \dots \\
a_{11} & \dots & a_{nn}\\
\end{bmatrix}
=
det
\begin{bmatrix}
a_{11} & \dots & a_{1n}\\
\dots & \dots & \dots \\
b_{i1} & \dots & b_{in} \\
\dots & \dots & \dots \\
a_{11} & \dots & a_{nn}\\
\end{bmatrix}
+
det
\begin{bmatrix}
a_{11} & \dots & a_{1n}\\
\dots & \dots & \dots \\
c_{i1} & \dots & c_{in}\\
\dots & \dots & \dots \\
a_{11} & \dots & a_{nn}\\
\end{bmatrix}
$$
\end{itemize}
Com $i,j$ duas linhas (ou colunas) \underline{distintas}, o impacto das operações elementares é:
\begin{enumerate}
\item O determinante após $\overrightarrow{\quad l_i \leftrightarrow l_j \quad}$ é o simétrico:
$$ A {\scriptstyle{\overrightarrow{\quad l_i \leftrightarrow l_j \quad}}} B, \quad det A = - det B$$
\item O determinante após $\overrightarrow{\quad \alpha \times l_i \quad}$ multiplica por $\alpha$:
$$ A {\scriptstyle{\overrightarrow{\quad \alpha \times l_i \quad}}} B, \quad \alpha \times det A = det B$$
\item O determinante após $\overrightarrow{\quad l_i + \alpha l_j \quad}$ mantem-se inalterado:
$$ A {\scriptstyle{\overrightarrow{\quad l_i + \alpha l_j \quad}}} B, \quad  det A = det B$$
\end{enumerate}
A partir do impacto das operações elementares no calculo do determinante pode concluir-se:
\begin{itemize}
\item Por base na segunda alínea, durante o calculo de um determinante as linhas e colunas podem ser postas em evidência:
$$\begin{vmatrix}
2 & 4 & 6 \\
3 & 9 & 15 \\
5 & 0 & 5
\end{vmatrix}
= 2
\begin{vmatrix}
1 & 2 & 3 \\
3 & 9 & 15 \\
5 & 0 & 5
\end{vmatrix}
= 2 \times 3
\begin{vmatrix}
1 & 2 & 3 \\
1 & 3 & 5 \\
5 & 0 & 5
\end{vmatrix}
= 2 \times 3 \times 5
\begin{vmatrix}
1 & 2 & 3 \\
1 & 3 & 5 \\
1 & 0 & 1
\end{vmatrix}
$$
\item $det(\alpha A) = \alpha^n det A$ (com $n$ o lado da matriz) visto que todas as linhas/colunas podem ter $\alpha$ em evidência.
\item Determinantes nulos após operações elementares implicam que a matriz original tinha determinante nulo.
\item A matriz $A$ só é invertível se $det A \neq 0$.\\
Ser invertível implica $r(A) = n$, o que implica que existe uma matriz triangular superior equivalente a $A$.\\
Em triangulares superiores o calculo do determinante é exequível com o produto dos elementos da diagonal.\\
Para o determinante ser nulo um ou mais desses elementos tem de ser nulos, o que implica $r(A) \neq n$.
\end{itemize}
O determinante do produto de duas matrizes é o produto do determinante dessas duas matrizes:
$$det(AB) = det(A)det(B)$$
O determinante da inversa é o inverso do determinante (relembrar: determinante nulo implica que não existe inversa):
$$det A^{-1} = \frac{1}{det A}$$
\section{Matriz adjunta}
A matriz adjunta, representada $adj A$ é dada pela transporta da matriz dos complementos algébricos.
$$adj A = \hat A^T$$
A partir da adjunta é possível obter a inversa:
$$A^-1 = \frac{1}{det A} adj A$$
A adjunta verifica ainda a seguinte propriedade:
$$
A \times adj A =
\begin{bmatrix}
det A & 0 & 0\\
0 & \ddots & 0\\
0 & 0 & det A
\end{bmatrix}
$$
\section{Regra de Cramer}
Seja $AX=B$ um sistema de Cramer ($A \in \mathcal{M}_{n \times n}$ e invertível).\\[0.2cm]
\textbf{Notação}: $A_j$ é a matriz que se obtém de $A$  substituindo a coluna $j$ por $B$.\\[0.2cm]
A regra de Cramer é um método para a obtenção do valor de $X$ que satisfaz o sistema.\\
Pretende-se portanto obter: $X^T=[s_1, s_2, \dots, s_n]$.\\
A regra define que cada $s_i$ é dado pela formula:
$$s_j = \frac{det A_j}{det A}$$
A regra de Cramer não é particularmente rápida de aplicar sendo preferível o método que recorre á inversa.
\section{???}
Só existe uma única aplicação $D: \mathcal{M}_{n \times n}(\mathbb{C}) \rightarrow \mathbb{C}$ que satisfaz simultaneamente as propriedades:
(por continuar, página 153 do livro)
\section{???}
Permutação (Por continuar)
\chapter{Espaços}
\section{Espaço Vetorial}
Um \textbf{espaço vetorial} $E$ é todo o conjunto não vazio que verifica as operações:
\begin{itemize}
\item \textbf{Adição} - Associação de pares de elementos a um outro elemento, todos de $E$.\\
Representada $a+b=c$ com $a,b,c \in E$\\
Esta adição tem de verificar associatividade, comutatividade, elemento neutro e elemento simétrico.
\item \textbf{Multiplicação externa} - Associação de um numero real/complexo a um elemento de $E$.\\
Representada $\alpha \cdot d$ com $\alpha \in \mathbb{C}, d \in E$\\
Esta multiplicação tem de verificar distributividade, associatividade e tem de ter um elemento neutro.
\end{itemize}
Um espaço vetorial diz-se \textbf{sobre} outro espaço vetorial quando a sua construção depende desse espaço.
\begin{itemize}
\item $\mathbb{R, C, R}^n$ são espaços vetoriais sobre $\mathbb{R}$.
\item $\mathcal{M}_{m \times n}(\mathbb{R})$ é um espaço vetorial sobre $\mathbb{R}$.
\item $\mathbb{R}_n[x]$ é o espaço com todos os polinómios de grau $<n\in \mathbb{N}_0$ na variável $x$ e um espaço vetorial sobre $\mathbb{R}$.
\end{itemize}
\section{Subespaço Vetorial}
Um \textbf{subespaço} é um conjunto de elementos de um espaço que só por si conseguem verificar a adição e multiplicação externa e o que isso implica (como a existência de um elemento neutro) em todos os seus elementos.\\
Qualquer ponto, reta ,plano ou hiperplano que passe na origem de um espaço é um subespaço.
\section{Combinação Linear de Vetores}
Um espaço é \textbf{combinação linear de vetores} se para um conjunto de vetores \underline{nele contidos} existirem coeficientes que permitam representar a totalidade do espaço.\\[2mm]
Um subespaço é dito \textbf{gerado} quando existe uma \textbf{sequência geradora}, uma combinação linear de vetores.
$$\left\langle u_1, u_2, \dots, u_n \right\rangle = \left\lbrace \alpha_1 u_1, \alpha_2 u_2, \dots, \alpha_n u_n \right\rbrace$$
Exemplos:\\
Os vetores unitários $\hat{i}$ e $\hat{j}$ são uma sequência geradora porque qualquer ponto pode ser dado por eles.\\
$\hat{i} = (1, 0)$ e $\hat{j} = (0, 1)$ define os vetores, logo a sequência geradora é definida: $\mathbb{R}^2 = \left\langle(1,0),(0,1)\right\rangle$\\[2mm]
Defina-se que $e_i$ é um vetor nulo exceto na posição $i$, na qual tem ``1" (exemplo: $e_3 = (0, 0, 1, \dots)$)\\
A sequência geradora para qualquer real com $n$ dimensões é $\mathbb{R}^n=\left\langle e_1, \dots, e_n\right\rangle$.\\[2mm]
Identicamente defina-se que $E_{ij}$ é a matriz nula exceto na posição $i,j$ na qual tem  ``1".\\
A sequência geradora para qualquer matriz é $\mathcal{M}_{m, n}=\left\langle E_{11}, \dots,  E_{1n}, E_{21}, \dots, E_{2n}, \dots, E_{m1}, \dots, E_{mn} \right\rangle$.\\[5mm]
Um espaço diz-se \textbf{finitamente gerado} se existir um numero finito de vetores na sua sequência geradora.\\
Exemplo: $\mathbb{R}_n[x]$ é finitamente gerado, mas $\mathbb{R}[x]$ já não.\\[2mm]
Podem ser efetuadas algumas operações nas sequências geradoras que não alteram o espaço gerado:
\begin{itemize}
\item Trocar a posição de dois vetores da sequência.
\item Multiplicar um vetor da sequência por um escalar.
\item Substituir um vetor da sequência pela sua soma com um múltiplo de outro vetor da sequência.
\end{itemize}
\section{Independência Linear}
Uma sequência geradora é \textbf{linearmente independente} quando nenhum dos seus vetores é redundante na definição do espaço.\\
Imagine-se que a $\hat{i}$ e $\hat{j}$ se junta $\hat{z}$, um vetor arbitrário com duas dimensões. Não há nenhum vetor que possa ser feito com uma combinação linear destes três vetores que não pudesse ser feita com uma combinação linear de $\hat{i}$ e $\hat{j}$, pelo que a sequência é \textbf{linearmente dependente}.\\[2mm]
Uma sequência com um vetor é linearmente dependente se esse vetor é o elemento nulo do espaço.\\
Uma sequência com múltiplos vetores é linearmente dependente se um deles for combinação linear dos restantes.\\
Pelo contrario, se numa sequência nenhum dos vetores for combinação linear dos restantes, a sequência é linearmente independente.
\section{Bases}
$(u1, \dots, u_n)$ é uma \textbf{base} de um espaço se é uma das suas sequências geradoras e for linearmente independente.\\
Todas as bases de um espaço tem o mesmo numero de elementos, o mínimo admissível para a definição do espaço.\\
Um espaço $E$ cujas bases tem $n$ elementos tem \textbf{dimensão} $n$ é denotado $dim E = n$.\\
Espaços que não são finitamente gerados tem \textbf{dimensão infinita}.\\[2mm]
Exemplos:\\
$dim \mathbb{R}^n = n \quad \quad
dim \mathcal{M}_{m, n} = mn \quad \quad
dim \mathbb{R}_n[x] = n+1 \quad \quad
dim \mathbb{C} = n$
\subsection{Propriedades}
\begin{itemize}
\item Todas as sequências geradoras tem uma subsequência que é base do mesmo espaço.
\item Uma sequência de $r$ vetores linearmente independentes pode ser base de um espaço de dimensão $n$ (com $r<n$) perante a adição de outros $n-r$ vetores á sequência.
\item  Dois espaços terem a mesma dimensão não implica serem iguais. (exemplo: $dim \mathbb{R}^4 = dim \mathcal{M}_{2, 2} $)
\item  Um subespaço tem sempre uma dimensão menor ou igual que o espaço original.\\
Um subespaço é igual ao espaço original se a suas dimensões forem as mesmas.
\item  A uma combinação de coeficientes de uma base chama-se de \textbf{coordenadas} e resultam num vetor único, em que mais nenhuma combinação resulta.\\
Pelo mesmo motivo, todos os vetores de um espaço são dados por aplicação (única) de coeficientes na sua base.
\item \textbf{Base canónica} é a base cujos vetores tem 1 num dos seus componentes e os restantes componentes nulos.\\
Exemplo: $\left\langle (1, 0, 0), (0, 1, 0), (0, 0, 1) \right\rangle$
\end{itemize}
\section{Interseção, União e Soma de Subespaços}
A \textbf{interseção} de sub-espaços de um espaço também é um sub-espaço desse espaço.\\[2mm]
A \textbf{união} de subespaços de um espaço \underline{não} é um subespaço desse espaço.\\
No entanto, se um dos subespaços estiver contido noutro, a união já é um subespaço.\\
Exemplo: A união dos eixos coordenados permite qualquer ponto fora do espaço dos mesmos.\\[2mm]
A \textbf{soma} de dois subespaços de um espaço também é um subespaço desse espaço.\\[2mm]
Sejam $F, G$ subespaços de um espaço $E$.
\begin{itemize}
\item $(f_1, \dots, f_r , g_1, \dots, g_s)$ é uma base e $F \cap G$ é nulo. (? algo estranho aqui!)
\item A soma $F+G$ é \textbf{direta} se cada vetor resultante existir um único par que o constituiu: $(\forall w \in F+G) \Rightarrow (\exists^1 (f, g))$\\
Nesse caso a soma é representada $F \oplus G$.
\item Os subespaços são \textbf{subespaços suplementares} se a soma direta resulta no espaço ($E = F \oplus G$).\\
Todo o subespaço de dimensão finita tem um suplementar.
\item $dim(F + G) = dim F + dim G - dim (F \cap G)$
\end{itemize}
\section{Matrizes e espaços vetoriais}
As sequências geradoras de um espaço podem ser representadas matricialmente. (?)\\
Uma sequência $(u_1, \dots, u_p)$ é linearmente independente só se a caraterística da matriz $[u_1 \dots u_p]^T$ for $p$.
$$r \left( \begin{bmatrix}
u_1 \\ \vdots \\ u_p
\end{bmatrix} \right)
= p$$
As linhas não nulas de uma matriz em forma de escada são portanto linearmente independentes.\\
Um vetor $\vec v$ só pertence a uma sequência geradora $\left\langle u_1, \dots, u_p \right\rangle$ se:
$$r \left( \begin{bmatrix}
u_1 \\ \vdots \\ u_p
\end{bmatrix} \right)
= r \left( \begin{bmatrix}
u_1 \\ \vdots \\ u_p \\ v
\end{bmatrix} \right)$$
(por completar, página 251)
\vspace{5mm}
Seja $A \in \mathcal{M}_{m \times n}$.\\
Designa-se \textbf{espaço de linhas} de $A$ o subespaço gerado pelas linhas de $A$. É representado $L(A)$.\\
O \textbf{espaço de colunas} é similar, nesse caso representa-se $C(A)$.\\
A $dim L(A)$ e $dim C(A)$ designa-se \textbf{caraterística de linha} e \textbf{caraterística de coluna} respetivamente.\\[2mm]
Seja $A$ uma matriz e $B$ uma transformada por linhas sua.\\
$(L(A) = L(B))$ mas $(C(A) = C(B))$
As transformações por linha não alteram a caraterística de coluna de $A$.\\
A caraterística de linha e a caraterística de coluna de $A$ são iguais e coincidem com a caraterística de $A$.
$dim L(A) = dim C(A) = r(A)$\\
As seguintes afirmações são todas equivalentes. A caraterística de uma matriz $A$ é:
\begin{itemize}
\item O numero de linhas não nulas de qualquer matriz equivalente por linhas a $A$ em forma de escada.
\item A dimensão do espaço das linhas de $A$.
\item A dimensão do espaço das colunas de $A$.
\item O numero máximo de linhas linearmente independentes de $A$.
\item O numero máximo de colunas linearmente independentes de $A$.
\end{itemize}
Havendo uma matriz $A \in \mathcal{M}_{m \times n}$, o subespaço de $\mathbb{R}^n$ com vetores que são soluções do sistema $AX=0$ é designado \textbf{espaço nulo} ou \textbf{núcleo} e representa-se $N(A)$.
$$N(A) = \left\lbrace
(a_1, \dots, a_n) \in \mathbb{R}^n :
A \begin{bmatrix}
\alpha_1 \\ \vdots \\ \alpha_n
\end{bmatrix} = 0
\right\rbrace$$
A dimensão do espaço nulo, $dim N(A)$, é designada \textbf{nulidade} e é igual ao grau de indeterminação do sistema $AX=0$.
$$dim N(A) = n - r(A)$$
A sequência geradora do espaço nulo é linearmente independente.
\section{Aplicações Lineares}
$f:E \to E'$ é \textbf{aplicação linear} se os elementos de $E$ verificarem as propriedades da soma e multiplicação externa:
$$\forall_{u,v \in E} \> f(u+v) = f(u) + f(v) \quad \quad \quad
\forall_{\alpha \in \mathbb{C}}, \forall_{u \in E} \> f(\alpha u) = \alpha f(u)$$
Pela definição obtém-se os seguintes axiomas:
\begin{itemize}
\item Uma aplicação $f: \mathbb{R} \to \mathbb{R}, f(x) = mx + b$ é linear tiver $b=0$.
\item $\forall_{u \in E} f(-u) = -f(u)$
\end{itemize}
\vspace{2mm}
Seja $\beta \in \mathbb{R}$ e $f:E \to E$.
\begin{itemize}
\item Se $\forall_{w \in E} \> f(w) = \beta w$ então $f$ é \textbf{homotetia} de razão $\beta$.
\item Se $\forall_{w \in E} \> f(w) = 0_E$ então $f$ é uma \textbf{aplicação nula}.
\item Se $\forall_{w \in E} \> f(w) = w$ então $f$ é uma \textbf{aplicação identidade} (representada $id_E(W)$).
\end{itemize}
Aplicações notaveis:
\begin{itemize}
\item $(f+g)(u) = f(u)+g(u)$ é uma \textbf{aplicação soma}.
\item $(\alpha f)(u) = \alpha f(u)$ é uma \textbf{aplicação produto}.
\item Se $f:A \to B$ e $g:B \to C$ então a \textbf{aplicação composta} ``$g$ após $f$" é representada $(g \circ f)(a) = g(f(a))$.\\
Uma aplicação que seja a composição de aplicações lineares também é linear.
\end{itemize}
Seja $f:E \to E$:\\
$f^k: E \to E$ é designada \textbf{potência de exponente $k$}.\\
Uma potência exponente é definida: $f^k = f^{k-1} \circ f$ exceto em $k=0$ para o qual é $id_E$.\\
A $f(E)$ (ou $Im \> f$), o conjunto resultante da aplicação designa-se de \textbf{contradomínio} ou \textbf{imagem}.\\[2mm]
Seja $f:E \to E'$:\\
O \textbf{núcleo} de uma aplicação são todos os valores $a$ tais que $f(a) = 0_{E'}$.\\
Um núcleo designa-se $Nuc \> f$ e tem sempre o elemento $0_E$.\\
O núcleo é um subespaço de $E$ e a imagem um subespaço de $E'$.\\[2mm]
A \textbf{imagem reciproca} (ou \textbf{imagem inversa}) é o elemento do conjunto de partida que corresponde a um elemento do conjunto de chegada\\
A aplicação inversa é a que obtém as imagens inversas: $f^{\leftarrow}(E') = \{u \in E : f(u) \in E'\}$\\[5mm]
Uma aplicação é:
\begin{itemize}
\item \textbf{Sobrejetiva} - se toda a imagem tem uma inversa na conjunto de partida.
\item \textbf{Injetiva} - Se cada valor tiver uma imagem distinta.\\
Uma aplicação injetiva tem núcleo $\{0_E\}$.
\item \textbf{Bijetiva} - Se cada valor tiver uma imagem distinta, e todas as imagens tem valor.\\
Ou seja: Injetiva e sobrejetiva.
\end{itemize}
Seja $E=(v_1, \dots, v_n)$ um espaço, $f$ uma aplicação linear finitamente gerada:\\
O espaço da imagem é dado pela aplicação nos vetores da base: $Im \> f = \left\langle f(v_1), \dots, f(v_n) \right\rangle$\\
Se os vetores $(u_1, \dots, u_n) \in E$ forem linearmente independentes, e $f$ for injetiva, então $(f(u_1), \dots, f(u_n))$ são linearmente independentes.\\
Se $f$ injetiva a dimensão do conjunto de partida é a dimensão do conjunto de chegada: $dim\> W = dim\> f(W)$.\\[2mm]
No contexto de uma aplicação ($f$):
\begin{itemize}
\item \textbf{Nulidade} - A dimensão do núcleo, representa-se $n(f)$.
\item \textbf{Caraterística} - A dimensão da imagem da imagem, representa-se $c(f)$.
\end{itemize}
Seja $f:E \to E'$:
\begin{itemize}
\item $dim E = dim \> Nuc \> F + dim \> Im \> F$
\item Se $dim E < dim E'$ a aplicação não é sobrejetiva.
\item Se $dim E > dim E'$ a aplicação não é injetiva.
\item Se $dim E = dim E'$ a aplicação é bijetiva.
\end{itemize}
Sejam $E$ e $E'$ espaços vetoriais, $B=(u_1, \dots, u_n)$ uma base de $E$ e $(v_1, \dots, v_n)$ vetores arbitrários de $E'$.\\
Só existe uma aplicação $f:E \to e$ que verifica $f(u_1) = v_1, \dots, f(u_n) = v_n$.\\[2mm]
Uma aplicação é \textbf{invertível} se existir outra aplicação tal que a composta de ambas resulte na aplicação identidade.\\
I.E: $f:A\to B$ invertível se $\exists^1 (f^{-1}:B\to A) | (f^{-1} \circ f) = id(A)$.\\[2mm]
Uma aplicação tem, \textbf{isomorfismo linear} quando é linear e bijetiva.\\
Perante uma aplicação isomorfa $f: E \to E'$, $E$ diz-se \textbf{isomorfo} a $E'$ e a relação representa-se $E \simeq E'$.\\
O isomorfismo é comutativo e transitivo. Seja $E''$ um espaço arbitrário:
\begin{itemize}
\item Se $E \simeq E'$ então $E' \simeq E$.
\item Se $E \simeq E'$ e $E' \simeq E''$ então $E \simeq E''$.
\item Se $E \simeq E'$ então $dim E = dim E'$\\
Exemplo: $\mathcal{M}_{2 \times 3} \simeq \mathbb{R}^6 \simeq \mathbb{R}_{5}[x]$
\end{itemize}
\subsection{Matrizes relacionais}
Seja $F: E \to E'$ uma aplicação linear, $B=(u_1, \dots, u_m)$ uma base de $E$ e $B'=(v_1, \dots, v_n)$ uma base de $E'$:\\
Se $A$ for uma matriz na qual cada coluna $(a_{1j}, \dots, a_{mj})$ corresponde á sequência de coordenadas de $f(u_j)$ (na base $B'$), então é designada \textbf{matriz de $f$ em relação ás bases $B$ e $B'$}, e representada $\mathcal{M}(f; B, B')$.\\
A caraterística de $A$ corresponde á dimensão da $B'$: $r(R) = dim \> Im \> f$.\\[2mm]
Com a aplicação $f: E \to E'$  e $u = (\alpha_1, \dots, \alpha_m) \in E$, a matriz $A = \mathcal{M}(f; B, B')$ serve para calcular $v = (\beta_1, \dots, \beta_n)$ o vetor resultante da aplicação a $u$: $f(u)$.
$$A
\begin{bmatrix}
\alpha_1 \\ \vdots \\ \alpha_m
\end{bmatrix}
=
\begin{bmatrix}
\beta_1 \\ \vdots \\ \beta_n
\end{bmatrix}$$
$\mathcal{M}(id_E; B, B')$, transforma uma coordenada numa base para outra coordenada noutra base no mesmo espaço.\\
Designa-se de \textbf{matriz de mudança de base}.\\[2mm]
Sejam $f: E \to E'$, $g: E \to E'$ aplicações lineares, $B, B'$ bases de $E, E'$ e $F = M(f: B, B')$ e $G = \mathcal{M}(g: B, B')$.\\
Verificam-se as propriedades:
\begin{itemize}
\item $\mathcal{M}(f + g: B, B') = F + G$
\item $\mathcal{M}(\alpha f: B, B') = \alpha F$
\end{itemize}
\vspace{2mm}
Sejam $f: E \to E'$, $g: E' \to E''$ duas aplicações lineares, e $B, B', B''$ as bases dos espaços, respetivamente:\\
Se $F = \mathcal{M}(f: B, B')$ e $G = \mathcal{M}(f: B, B')$ então $\mathcal{M}(g \circ f: B, B'') = GF$.\\
Ou seja $\mathcal{M}(f: B, B') \mathcal{M}(f: B, B') = \mathcal{M}(g \circ f: B, B'')$.\\[5mm]
Se uma aplicação $f: E \to E'$ dada pela matriz $A$ for invertível, então $A^{-1} = \mathcal{M}(f^{-1}; B', B)$.\\[5mm]
Toda a matriz de mudança de base é invertível $\left(\mathcal{M}(id_E; B, B')\right)^{-1} = \mathcal{M}(id_E; B', B)$.\\[5mm]
Sejam $f: E \to E'$, $g: E' \to E''$ uma aplicação linear, $B_1, B_1$ bases de $E$ e $B_1', B_2'$ bases de $E'$:\\
Se $A_1 = \mathcal{M}(f; B_1, B_1')$,  $A_2 = \mathcal{M}(f; B_2, B_2')$, $Q = \mathcal{M}(id_E; B_1', B_2')$ e  $P = \mathcal{M}(id_E; B_2, B_1)$ então:
\begin{itemize}
\item $\mathcal{M}(f, B_1, B_2') = QA_1$
\item $\mathcal{M}(f, B_2, B_1') = A_1P$
\item $A_2 = QA_1 P$
\end{itemize}
Sejam $A_1, A_2 \in \mathcal{M}_{m \times n}, Q \in \mathcal{M}_{n \times m}$ e $P \in \mathcal{M}_{n \times n}$:
\begin{itemize}
\item $A_1$ é \textbf{equivalente} a $A_2$ se $Q, P$ são invertíveis e $A_2 = QA_1P$.
\item Se $A_1$ equivalente a $A_2$ então $A_2$ é equivalente a $A_1$.
\item $A_1$ é \textbf{semelhante} a $A_2$ se $\exists P^{-1}$ que verifique $A_2 = P^{-1}A_1P$.
\end{itemize}
\clearpage
\section{Vetores próprios}
\textbf{Definição informal}: Um \textbf{vetor próprio}(também conhecido como \textit{eigenvector}) de uma transformação é o vetor que não sofre transformação alguma alem da mudança de magnitude. Não é deslocado ou rodado.\\
O \textbf{valor próprio} é a proporção que um vetor próprio tem em relação ao seu original antes da transformação, ou seja, o escalamento que sofreu.\\
Um \textbf{subespaço próprio} é o subespaço em que um vetor próprio está confinado, ou seja, uma reta.\\[5mm]
Seja $A \in \mathcal{M}_{n \times n}$. Se um vetor $X$ e um escalar $\alpha$ verificam $AX = \alpha X$ então:
\begin{itemize}
\item $\alpha$ é \textbf{valor próprio} da transformação $A$.
\item $X$ é \textbf{vetor próprio} de $A$ associado ao valor próprio $\alpha$.
\end{itemize}
Uma aplicação linear é \textbf{endomórfica} se tiver conjunto de partida igual á imagem ($f: E \to E$).\\
Se uma aplicação linear é endomórfica e verifica $f(\vec u)= \alpha \vec u$, então $\vec u$ é um \textbf{vetor próprio} $\alpha$ é o seu \textbf{valor próprio}.\\
$E_\alpha$ é um \textbf{subespaço próprio} de $E$ associado ao valor próprio $\alpha$, contem os vetores próprios com esse valor:
$$E_\alpha = \{\vec u \in E : f(\vec u) = \alpha \vec u\} = Nuc(f-\alpha id_E)$$
$E_\alpha$ sendo um subespaço de $E$ tem a dimensão limitada: $1 \leq dim E_\alpha \leq dim E$.\\[2mm]
Seja $A \in \mathcal{M}_{n \times n}$, e seja $\alpha$ um dos seus valores próprios:\\
$\mathcal{M}_\alpha = \{X \in \mathcal{M}_{n \times 1} : AX = \alpha X\}$ é um subespaço vetorial com os vetores próprios de $A$ e um vetor nulo $\{0_{n \times 1}\}$.\\
A dimensão de $\mathcal{M}_{\alpha}$ é designada \textbf{multiplicidade geométrica} do valor próprio $\alpha$ e representada $mg(\alpha)$.
$$mg(\alpha) = dim \mathcal{M}_\alpha \leq dim \mathcal{M}_{n \times 1} = n$$
Seja $A \in \mathcal{M}_{n \times n}$ e seja $\alpha$ um dos seus valores próprios.\\
$mg(\alpha) = n - r(A - \alpha I_n)$\\[2mm]
Seja $A \in \mathcal{M}_{n \times n}$ uma transformação e $X$ um vetor próprio com valor próprio $\alpha$:\\
A aplicação sucessiva da transformação $A$ é equivalente ao dimensionamento sucessivo do vetor próprio $X$.\\
Formalmente: $\forall k \in \mathbb{N} : A^k X = \alpha^k X$
\subsection{Teorema Fundamental da Álgebra}
Qualquer equação na variável $x$ na forma
$$a_n x^n + a_{n-1} x^{n-1} + \dots + a_1 x + a_0 = 0$$
com $a_n \neq 0$ e $a \subset \mathbb{C}$ tem exatamente $n$ zeros.
\subsection{Polinómio caraterístico}
Seja $A\in \mathcal{M}_{n \times n}$, $X$ um vetor próprio seu e $\alpha$ o seu valor próprio.\\[5mm]
Todo o polinómio  $q(x) = b_r x^r + b_{r-1} x^{r-1} + \dots + b_1 x + b_0 \in \mathbb{C}_r[x]$ quando definido por base em $A$:\\
.\hspace{25mm} $q(x) = b_r A^r + b_{r-1} A^{r-1} + \dots + b_1 A x + b_0 I_n$ verifica $q(A)X = q(\alpha)X$.\\
Alem disso, se $q(A) = 0$ então $q(\alpha) = 0$.\\[2mm]
$\alpha$ só é valor próprio de $A \in \mathcal{M}_{n \times n}$ se $|A - \alpha I_n| = 0$.\\
Chama-se \textbf{polinómio caraterístico} de $A$ ao polinómio na variável $x$ dado por:
$$p_A(x) = |A - x I_n|$$
$p_A(x) = 0$ é a \textbf{equação caraterística} de $A$.\\
O seu polinómio caraterístico de $A$ é:
$$p_A(x) = (-1)^n x^n + a_{n-1} x ^{n-1} + \dots + a_1 x + a_0$$
Com $a_i \in \mathbb{R}$ para $i=1, \dots , n-1$ e $a_0 = det A$.\\
Os valores próprios de $A$ são os zeros do polinómio caraterístico.\\[2mm]
À multiplicidade de $\alpha$ enquanto zero do polinómio caraterístico designa-se de \textbf{multiplicidade algébrica}, representa-se $ma(\alpha)$ e  pode ser interpretada como o maior $k$ tal que $(\alpha - x)^k$ divide $p_A(x)$.\\[2mm]
Se $\beta = (\beta_1, \dots, \beta_n)$ forem os zeros do polinómio caraterístico de $A$ então $det A = \prod \beta$.\\[2mm]
Os valores próprios de uma matriz diagonal triangular são os elementos da diagonal principal.\\[2mm]
As seguintes preposições são equivalentes:
\begin{itemize}
\item $A$ é invertível ($det A \neq 0$).
\item $A$ não tem valor próprio $0$ ($|A - 0 I_n| \neq 0$).
\item O termo constante do polinómio caraterístico de $A$ não é nulo.
\end{itemize}
A multiplicidade geométrica é sempre menor ou igual à multiplicidade algébrica. $mg(\alpha) \leq ma(\alpha)$\\[2mm]
Sejam $\alpha, \beta$ valores próprios de $A$ com $\alpha \neq \beta$:\\
Quaisquer vetores próprios com valor próprio $\alpha$ que sejam linear independentes entre si, são também linearmente independentes dos vetores próprios de valor $\beta$.
\subsection{Diagonalização}
Seja $A\in \mathcal{M}_{n \times n}$, $\alpha$ um valor próprio $E$ é um espaço com dimensão finita.\\[2mm]
$A$ é \textbf{diagonalizavel} se for semelhante a uma matriz diagonal, ou seja:
$$\exists P, D \in \mathcal{M}_{n \times n}: P^{-1}AP = D$$
Sendo $P$ invertível e $D$ matriz diagonal. $P$ é dita a \textbf{diagonalizante} de $A$.\\
Caso isto se verifique, então os valores próprios de $A$ são os elementos da diagonal principal de $D$.\\
Isso implica que $A$ só é diagonalizavel se tem $n$ vetores próprios linearmente independentes.\\[5mm]
Se $\vec u_1, \dots, \vec u_k \in \mathcal{M}_{m \times 1}$ são os vetores próprios linearmente independentes de $A$, correspondentes aos valores próprios $\alpha, \dots, \alpha_n$, então $P = [\vec u_1 | \dots | \vec u_n] $ é a matriz diagonalizante de $A$.
$$P^{-1}AP =
\begin{bmatrix}
\alpha_1 & 0 & \dots & 0 \\
0 & \alpha_2 & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \alpha_n \\
\end{bmatrix}$$
$A$ só é diagonalizavel se $\sum_{i=1}^r mg(\alpha_i) = n$ sendo $\alpha_1, \dots, \alpha_r$ os distintos valores próprios.\\[5mm]
Uma aplicação linear $f: E \to E$ diz-se diagonalizavel se existe uma base tal que $\mathcal{M}(f; B, B)$ é uma matriz diagonal.\\
Se $dim E = n$, então $f$ só é diagonalizavel se tem $n$ vetores próprios linearmente independentes.
\end{document}